{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01a_nlm_and_contextual_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3VSgZXfFdCk",
        "colab_type": "text"
      },
      "source": [
        "# Fine tuning pre-trained language models for text classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzrX_0SDViOu",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "Fine-tuning pre-trained language models learnt with transformers has improved the state of the art in multiple NLP evaluation tasks(see [SuperGlue leader board](https://super.gluebenchmark.com/leaderboard)). Learning a language model is an unsupervised task where the model learns to predict the next word in a sequence given the previous words. Neural language models have been implemented as feed foorward networks, LSTMS (ELMo, ULMFit), and transformers-encoders (BERT) or decoders (Open AI GPT).\n",
        "\n",
        "In this notebook we fine tune a BERT pre-trained language model to carry out a binary classification task where tweets are labelled as generated by bots or hurmans. \n",
        "\n",
        "<!-- The notebook is structured as follows:\n",
        "\n",
        "- Motivation\n",
        "- Setup\n",
        "  - Libraries required\n",
        "  - Dataset\n",
        "- A glimpse on BERT tokenization \n",
        "- Fine tune the model\n",
        "- Evaluate the BERT classifier \n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUlcUwnALTCn",
        "colab_type": "text"
      },
      "source": [
        "## Motivation\n",
        "While word embeddings are learnt from large corpora, their use in neural models to solve specific tasks is limited to the input layer. So in practice a task-specific neural model is built almost from scratch because most of the model parameters are initialized randomly, and hence, these paremeters need to be optimized for the task at hand, requiring large sets of data to produce a high performance model.\n",
        "\n",
        "Recent advances in neural language models (BERT or OPEN AI GPT) have shown evidence that task specific architectures are not longer necessary and transfering some internal representations (attention blocks) along with shallow feed forward networks is enough. \n",
        "\n",
        "In (Garcia et al.,2019) we presented an experimental study on the use of word embeddings as input of CNN architectures and Bi-LSTM to tackle the bot detection task and compare these results with fine-tuning pretrained language models. \n",
        "\n",
        "Evaluation results, presented in the figure below, show that fine-tuning language models yields overall better results than training specific neural architectures that are fed with mixture of: i) pre-trained contextualized word embeddings (ELMo), ii) pre-trained  context-indepedent word embeddings learnt from Common Crawl(FastText), Twitter (GloVe), and urban dictionary (word2vec), plus embeddings optimized by the neural network in the learning process. \n",
        "\n",
        "![Bot detection classification task](https://drive.google.com/uc?id=1rSzM544MK2QOezpvUKHfrxATbkEiyBHX)\n",
        "\n",
        "\n",
        "\n",
        "**References**\n",
        "\n",
        "Garcia-Silva, Andres, et al. \"An Empirical Study on Pre-trained Embeddings and Language Models for Bot Detection.\" Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019). 2019.\n",
        "\n",
        "To cite this paper use the following BibTex entry: \n",
        "\n",
        "```\n",
        "@inproceedings{garcia-silva-etal-2019-empirical,\n",
        "    title = \"An Empirical Study on Pre-trained Embeddings and Language Models for Bot Detection\",\n",
        "    author = \"Garcia-Silva, Andres  and\n",
        "      Berrio, Cristian  and\n",
        "      G{\\'o}mez-P{\\'e}rez, Jos{\\'e} Manuel\",\n",
        "    booktitle = \"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)\",\n",
        "    month = aug,\n",
        "    year = \"2019\",\n",
        "    address = \"Florence, Italy\",\n",
        "    publisher = \"Association for Computational Linguistics\",\n",
        "    url = \"https://www.aclweb.org/anthology/W19-4317\",\n",
        "    doi = \"10.18653/v1/W19-4317\",\n",
        "    pages = \"148--155\",\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POOq3acQMrKT",
        "colab_type": "text"
      },
      "source": [
        "# A glimpse on BERT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U6jzPnupzRr",
        "colab_type": "text"
      },
      "source": [
        "## Input representation\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1tZS7sszhNtT3m25EZJkjC9PjRZDEPKGy\" alt=\"Token embeddings, segment embeddings, and positional embeddings\" width=\"500\"/>\n",
        "\n",
        "Image source: \"Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta8RVpIpUiyf",
        "colab_type": "text"
      },
      "source": [
        "## Pre-training learning objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU-qjZNfODqh",
        "colab_type": "text"
      },
      "source": [
        "### Masked language model\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=11GJiHlDeoKsShOwSJvMTq3fc8E8GxV4V\" alt=\"Masked LM\" width=\"500\"/>\n",
        "\n",
        "Image Source: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPqLDsHCQh6U",
        "colab_type": "text"
      },
      "source": [
        "### Next Sentence Prediction\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1x0ckvgMwb5j3SfVNId-EyQgyDlt3C42h\" alt=\"next sentence prediction\" width=\"500\"/>\n",
        "\n",
        "Image source: http://jalammar.github.io/illustrated-bert/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pt2r_VzV1AS",
        "colab_type": "text"
      },
      "source": [
        "## Contextualized word embeddings\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1LozTCltkxXbkrE2M72r0lDUWEDXLEWZU\" alt=\"Contextualized embeddings\" width=\"600\"/>\n",
        "\n",
        "Image source: http://jalammar.github.io/illustrated-bert/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtN3rqyOOEbB",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "fine-tuning BERT for a task just requires to incorporate one  additional  output  layer,  so a minimal number of parameters need to be learned from scratch.\n",
        "\n",
        "In the figure below E represents the input embedding,Ti represents the contextual representation of token i, [CLS] is the special symbol for classification output, and [SEP] is the specialsymbol to separate non-consecutive token sequences\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1wZPwbMNtHwf8g-7phWxwtJCxnTfPj-Ux\" width=\"600\"/>\n",
        "\n",
        "Image source: Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
        "\n",
        "To **fine-tune BERT for a sequence classification task** the transfomer output for the CLS token is used as the sequence representation. The transfomer output for the CLS token is connected to a one layer feed forward network that predicts the classification labels. All the BERT parameters and the FF network are fine-tune jointly to maximize the log-probability of the correct label.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HR3iIvgVmnO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Experimental setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TZEolAIcM9_",
        "colab_type": "text"
      },
      "source": [
        "## Transformers library\n",
        "\n",
        "We use transformer from huggingface: https://github.com/huggingface/transformers\n",
        "\n",
        "\"Transformers provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGBVqmkNqD_z",
        "colab_type": "code",
        "outputId": "d5f9c5d1-e10f-402a-9ac5-78332bed074f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.1.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.83)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.17)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.11.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.17 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.17)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.17->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.17->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAB8VCbxkjr4",
        "colab_type": "text"
      },
      "source": [
        "## Dataset \n",
        "\n",
        "We use the bot detection dataset generated in (Garcia et al.,2019) that was built starting from an existing list of twitter accounts that were manually labelled as bots and humans. Then we use the twitter API to extract tweets from these account.  In total the dataset contains around 600K tweet, approximately half of them generated by bots, and the other half by humans. \n",
        "\n",
        "In this notebook we provide a complete version of the dataset (large) and a reduced one (small) to be able to run the notebook whithin the time frame, since **fine tuning BERT on the large version takes more than 5h**. \n",
        "\n",
        "- Large: 500k train and 100k test labeled tweets which is in the path: \"'/content/gdrive/My Drive/09_BERT/Large_Dataset/\"\n",
        "- Small: 1k train and 100 test labeled tweets which is in the path: \"'/content/gdrive/My Drive/09_BERT/Small_Dataset/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XViapGYvR01H",
        "colab_type": "text"
      },
      "source": [
        "### Downloading from Google Drive\n",
        "\n",
        "Let's download the datasets and the models from Google Drive, and then decompress the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jD9hGcViOTO",
        "colab_type": "code",
        "outputId": "a5a39fa8-51c3-4ef9-9669-69c26c0a2838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=12Hn0uGUHLjR2VDAV-uWysBMJmaUjHasA' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=12Hn0uGUHLjR2VDAV-uWysBMJmaUjHasA\" -O BERT.tar && rm -rf /tmp/cookies.txt\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-18 20:42:06--  https://docs.google.com/uc?export=download&confirm=yp60&id=12Hn0uGUHLjR2VDAV-uWysBMJmaUjHasA\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.206.138, 74.125.206.113, 74.125.206.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.206.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/77o28rbjjg3bd508t7t1o0afle0j335t/1574107200000/16197418968100245121/*/12Hn0uGUHLjR2VDAV-uWysBMJmaUjHasA?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2019-11-18 20:42:06--  https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/77o28rbjjg3bd508t7t1o0afle0j335t/1574107200000/16197418968100245121/*/12Hn0uGUHLjR2VDAV-uWysBMJmaUjHasA?e=download\n",
            "Resolving doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)... 173.194.76.132, 2a00:1450:400c:c00::84\n",
            "Connecting to doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)|173.194.76.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘BERT.tar’\n",
            "\n",
            "BERT.tar                [       <=>          ] 805.39M   147MB/s    in 6.1s    \n",
            "\n",
            "2019-11-18 20:42:13 (132 MB/s) - ‘BERT.tar’ saved [844511024]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7WOVkEBks-n",
        "colab_type": "code",
        "outputId": "1b6a588f-c275-4dd9-8c1c-affc989b40ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "!tar xzvf ./BERT.tar -C ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./09_BERT/\n",
            "./09_BERT/Test_Dataset/\n",
            "./09_BERT/Test_Dataset/cached_dev_Bert_Classifier_Large_128_cola\n",
            "./09_BERT/Test_Dataset/cached_dev_Bert_Classifier_128_cola\n",
            "./09_BERT/Test_Dataset/cached_dev_Bert_Classifier_small_128_cola\n",
            "./09_BERT/Test_Dataset/dev.tsv\n",
            "./09_BERT/Bert_Classifier_Large/\n",
            "./09_BERT/Bert_Classifier_Large/training_args.bin\n",
            "./09_BERT/Bert_Classifier_Large/added_tokens.json\n",
            "./09_BERT/Bert_Classifier_Large/vocab.txt\n",
            "./09_BERT/Bert_Classifier_Large/eval_results.txt\n",
            "./09_BERT/Bert_Classifier_Large/predictions.txt\n",
            "./09_BERT/Bert_Classifier_Large/pytorch_model.bin\n",
            "./09_BERT/Bert_Classifier_Large/tokenizer_config.json\n",
            "./09_BERT/Bert_Classifier_Large/special_tokens_map.json\n",
            "./09_BERT/Bert_Classifier_Large/config.json\n",
            "./09_BERT/Large_Dataset/\n",
            "./09_BERT/Large_Dataset/cached_dev_Bert_Classifier_Large_128_cola\n",
            "./09_BERT/Large_Dataset/train.tsv\n",
            "./09_BERT/Large_Dataset/dev.tsv\n",
            "./09_BERT/Small_Dataset/\n",
            "./09_BERT/Small_Dataset/cached_dev_bert-base-uncased_128_cola\n",
            "./09_BERT/Small_Dataset/cached_train_bert-base-uncased_128_cola\n",
            "./09_BERT/Small_Dataset/train.tsv\n",
            "./09_BERT/Small_Dataset/Bert_Classifier/\n",
            "./09_BERT/Small_Dataset/Bert_Classifier/training_args.bin\n",
            "./09_BERT/Small_Dataset/Bert_Classifier/added_tokens.json\n",
            "./09_BERT/Small_Dataset/Bert_Classifier/vocab.txt\n",
            "./09_BERT/Small_Dataset/Bert_Classifier/eval_results.txt\n",
            "./09_BERT/Small_Dataset/Bert_Classifier/predictions.txt\n",
            "./09_BERT/Small_Dataset/Bert_Classifier/pytorch_model.bin\n",
            "./09_BERT/Small_Dataset/Bert_Classifier/tokenizer_config.json\n",
            "./09_BERT/Small_Dataset/Bert_Classifier/special_tokens_map.json\n",
            "./09_BERT/Small_Dataset/Bert_Classifier/config.json\n",
            "./09_BERT/Small_Dataset/dev.tsv\n",
            "./09_BERT/run_glue.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Va-9Z80pAQr",
        "colab_type": "text"
      },
      "source": [
        "### Set the dataset version\n",
        "The enviroment variable DATA_DIR holds the path to the dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-Bg5I-VrylZ",
        "colab_type": "code",
        "outputId": "36445a92-cf5c-4dd9-8067-cbdcfc16cea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%env DATA_DIR=./09_BERT/Small_Dataset/\n",
        "\n",
        "#Uncomment the following line to use the large version of the dataset\n",
        "#%env DATA_DIR=./09_BERT/Large_Dataset/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: DATA_DIR=./09_BERT/Small_Dataset/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8WmMvszp3so",
        "colab_type": "text"
      },
      "source": [
        "### Inspect the dataset\n",
        "\n",
        "The dataset is in the tsv format expected by transfomer library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zymDOvKEcMi5",
        "colab_type": "code",
        "outputId": "083f3b5d-9428-454f-b1f1-04764d0181a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "test = pd.read_csv(os.environ[\"DATA_DIR\"] + \"dev.tsv\", header=None, sep = '\\t')\n",
        "data = pd.DataFrame(test)\n",
        "data.columns = [\"index\", \"label\", \"mark\", \"tweet\"]\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "      <th>mark</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>Now Playing: ♬ Dick Curless - Evil Hearted Me ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>Not only are you comfortably swaddled in secur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>Follow @iAmMySign !!!  Follow @iAmMySign our o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>These strawberry sandwich cookies are so easy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>Do These Two Lines Match Up On Your Hands Here...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>I’m sorry you hurt your first-grade teacher’s ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>96</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>#HometimeReading: If you’ve enjoyed #KewOrchid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>97</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>Miss_5_Thousand : All my afternoon plans just ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>98</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>A bunch of associates, that I hardly associate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>99</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>Chokehold - Underneath  https://t.co/Vkj2jrIFEb</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    index  label mark                                              tweet\n",
              "0       0      1    a  Now Playing: ♬ Dick Curless - Evil Hearted Me ...\n",
              "1       1      0    a  Not only are you comfortably swaddled in secur...\n",
              "2       2      1    a  Follow @iAmMySign !!!  Follow @iAmMySign our o...\n",
              "3       3      0    a  These strawberry sandwich cookies are so easy ...\n",
              "4       4      0    a  Do These Two Lines Match Up On Your Hands Here...\n",
              "..    ...    ...  ...                                                ...\n",
              "95     95      0    a  I’m sorry you hurt your first-grade teacher’s ...\n",
              "96     96      1    a  #HometimeReading: If you’ve enjoyed #KewOrchid...\n",
              "97     97      1    a  Miss_5_Thousand : All my afternoon plans just ...\n",
              "98     98      0    a  A bunch of associates, that I hardly associate...\n",
              "99     99      1    a    Chokehold - Underneath  https://t.co/Vkj2jrIFEb\n",
              "\n",
              "[100 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pOZytkUVDJt",
        "colab_type": "text"
      },
      "source": [
        "# Hands-on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVRF_BMtOFBK",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization \n",
        "\n",
        "Recent neural languages models use subword representations. ELMO relies on characters, Open AI GPT on byte pair encoding, and BERT on the word pieces algorithms. These **subword representations are combined when unseen words during training needs to be processed, hence avoiding the OOV problem**. \n",
        "\n",
        "BERT uses a 30k WordPieces vocabulary. \n",
        "\n",
        "Let us see how the BERT Tokenizer works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1Hb2SAeqKrE",
        "colab_type": "code",
        "outputId": "ef5b9f06-503d-460c-fc3b-dd1abe17e348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from transformers import *\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "text = input(\"Enter a word or a sentence: \")\n",
        "print(tokenizer.tokenize(text))\n",
        "print(tokenizer.encode(text))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a word or a sentence: gentlemen\n",
            "['gentlemen']\n",
            "[11218]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76AcSBFqFyXi",
        "colab_type": "text"
      },
      "source": [
        "## Fine-Tuning the model\n",
        "\n",
        "The next step would be to fine-tune the model.\n",
        "\n",
        "Running the following script you can fine-tune the model and perform evaluation. While doing the evaluation the classification of the tweets on the test set is saved in the predictions.txt file that we will use later.\n",
        "\n",
        "The most relevant parameters of the script are:\n",
        "  - model type: the model that we are going to use, in this case BERT\n",
        "  - model name or path: the name of the model or path storing a specific model.\n",
        "  - task name: the task that we want to perform, in this case CoLA because we want to do classification.\n",
        "  - ouput dir: the directory in which it stores the fine-tuned model.\n",
        "  \n",
        "You can try to change the parameters and see how it affects performance. \n",
        "\n",
        "This process is slow even though we reduced the dataset. You should expect that it takes around 1 minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE6sLu94qSvR",
        "colab_type": "code",
        "outputId": "f410c988-a3fc-4c4b-bbdc-8df9ba72c7e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ./09_BERT/run_glue.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-uncased \\\n",
        "    --task_name CoLA \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_lower_case \\\n",
        "    --data_dir \"$DATA_DIR\" \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_eval_batch_size=8   \\\n",
        "    --per_gpu_train_batch_size=8   \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 1.0 \\\n",
        "    --save_steps 62500 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --output_dir  ./Bert_Classifier/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/18/2019 20:43:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/18/2019 20:43:57 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpv7l7b_tx\n",
            "100% 313/313 [00:00<00:00, 198490.65B/s]\n",
            "11/18/2019 20:43:57 - INFO - transformers.file_utils -   copying /tmp/tmpv7l7b_tx to cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
            "11/18/2019 20:43:57 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
            "11/18/2019 20:43:57 - INFO - transformers.file_utils -   removing temp file /tmp/tmpv7l7b_tx\n",
            "11/18/2019 20:43:57 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
            "11/18/2019 20:43:57 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/18/2019 20:43:57 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "11/18/2019 20:43:58 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpmgj3j9f7\n",
            "100% 440473133/440473133 [00:16<00:00, 26026226.54B/s]\n",
            "11/18/2019 20:44:15 - INFO - transformers.file_utils -   copying /tmp/tmpmgj3j9f7 to cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "11/18/2019 20:44:16 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "11/18/2019 20:44:16 - INFO - transformers.file_utils -   removing temp file /tmp/tmpmgj3j9f7\n",
            "11/18/2019 20:44:16 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "11/18/2019 20:44:20 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "11/18/2019 20:44:20 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "11/18/2019 20:44:26 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='./09_BERT/Small_Dataset/', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='./Bert_Classifier/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=62500, seed=42, server_ip='', server_port='', task_name='cola', tokenizer_name='', tpu=False, tpu_ip_address='', tpu_name='', warmup_steps=0, weight_decay=0.0, xrt_tpu_config='')\n",
            "11/18/2019 20:44:26 - INFO - __main__ -   Loading features from cached file ./09_BERT/Small_Dataset/cached_train_bert-base-uncased_128_cola\n",
            "11/18/2019 20:44:26 - INFO - __main__ -   ***** Running training *****\n",
            "11/18/2019 20:44:26 - INFO - __main__ -     Num examples = 1000\n",
            "11/18/2019 20:44:26 - INFO - __main__ -     Num Epochs = 1\n",
            "11/18/2019 20:44:26 - INFO - __main__ -     Instantaneous batch size per GPU = 8\n",
            "11/18/2019 20:44:26 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "11/18/2019 20:44:26 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "11/18/2019 20:44:26 - INFO - __main__ -     Total optimization steps = 125\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/125 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/125 [00:00<01:11,  1.73it/s]\u001b[A\n",
            "Iteration:   2% 2/125 [00:01<01:05,  1.87it/s]\u001b[A\n",
            "Iteration:   2% 3/125 [00:01<01:02,  1.97it/s]\u001b[A\n",
            "Iteration:   3% 4/125 [00:01<00:58,  2.07it/s]\u001b[A\n",
            "Iteration:   4% 5/125 [00:02<00:55,  2.15it/s]\u001b[A\n",
            "Iteration:   5% 6/125 [00:02<00:53,  2.21it/s]\u001b[A\n",
            "Iteration:   6% 7/125 [00:03<00:52,  2.26it/s]\u001b[A\n",
            "Iteration:   6% 8/125 [00:03<00:50,  2.29it/s]\u001b[A\n",
            "Iteration:   7% 9/125 [00:03<00:49,  2.32it/s]\u001b[A\n",
            "Iteration:   8% 10/125 [00:04<00:49,  2.34it/s]\u001b[A\n",
            "Iteration:   9% 11/125 [00:04<00:48,  2.36it/s]\u001b[A\n",
            "Iteration:  10% 12/125 [00:05<00:47,  2.36it/s]\u001b[A\n",
            "Iteration:  10% 13/125 [00:05<00:47,  2.37it/s]\u001b[A\n",
            "Iteration:  11% 14/125 [00:06<00:46,  2.37it/s]\u001b[A\n",
            "Iteration:  12% 15/125 [00:06<00:46,  2.37it/s]\u001b[A\n",
            "Iteration:  13% 16/125 [00:06<00:45,  2.38it/s]\u001b[A\n",
            "Iteration:  14% 17/125 [00:07<00:45,  2.38it/s]\u001b[A\n",
            "Iteration:  14% 18/125 [00:07<00:44,  2.38it/s]\u001b[A\n",
            "Iteration:  15% 19/125 [00:08<00:44,  2.38it/s]\u001b[A\n",
            "Iteration:  16% 20/125 [00:08<00:44,  2.39it/s]\u001b[A\n",
            "Iteration:  17% 21/125 [00:09<00:43,  2.38it/s]\u001b[A\n",
            "Iteration:  18% 22/125 [00:09<00:43,  2.38it/s]\u001b[A\n",
            "Iteration:  18% 23/125 [00:09<00:42,  2.38it/s]\u001b[A\n",
            "Iteration:  19% 24/125 [00:10<00:42,  2.38it/s]\u001b[A\n",
            "Iteration:  20% 25/125 [00:10<00:41,  2.39it/s]\u001b[A\n",
            "Iteration:  21% 26/125 [00:11<00:41,  2.39it/s]\u001b[A\n",
            "Iteration:  22% 27/125 [00:11<00:41,  2.37it/s]\u001b[A\n",
            "Iteration:  22% 28/125 [00:11<00:40,  2.38it/s]\u001b[A\n",
            "Iteration:  23% 29/125 [00:12<00:40,  2.37it/s]\u001b[A\n",
            "Iteration:  24% 30/125 [00:12<00:39,  2.38it/s]\u001b[A\n",
            "Iteration:  25% 31/125 [00:13<00:39,  2.38it/s]\u001b[A\n",
            "Iteration:  26% 32/125 [00:13<00:39,  2.38it/s]\u001b[A\n",
            "Iteration:  26% 33/125 [00:14<00:38,  2.38it/s]\u001b[A\n",
            "Iteration:  27% 34/125 [00:14<00:38,  2.38it/s]\u001b[A\n",
            "Iteration:  28% 35/125 [00:14<00:37,  2.38it/s]\u001b[A\n",
            "Iteration:  29% 36/125 [00:15<00:37,  2.38it/s]\u001b[A\n",
            "Iteration:  30% 37/125 [00:15<00:36,  2.38it/s]\u001b[A\n",
            "Iteration:  30% 38/125 [00:16<00:36,  2.37it/s]\u001b[A\n",
            "Iteration:  31% 39/125 [00:16<00:36,  2.38it/s]\u001b[A\n",
            "Iteration:  32% 40/125 [00:17<00:35,  2.38it/s]\u001b[A\n",
            "Iteration:  33% 41/125 [00:17<00:35,  2.38it/s]\u001b[A\n",
            "Iteration:  34% 42/125 [00:17<00:34,  2.38it/s]\u001b[A\n",
            "Iteration:  34% 43/125 [00:18<00:34,  2.37it/s]\u001b[A\n",
            "Iteration:  35% 44/125 [00:18<00:34,  2.37it/s]\u001b[A\n",
            "Iteration:  36% 45/125 [00:19<00:33,  2.37it/s]\u001b[A\n",
            "Iteration:  37% 46/125 [00:19<00:33,  2.38it/s]\u001b[A\n",
            "Iteration:  38% 47/125 [00:19<00:32,  2.38it/s]\u001b[A\n",
            "Iteration:  38% 48/125 [00:20<00:32,  2.38it/s]\u001b[A\n",
            "Iteration:  39% 49/125 [00:20<00:31,  2.38it/s]\u001b[A\n",
            "Iteration:  40% 50/125 [00:21<00:31,  2.38it/s]\u001b[A\n",
            "Iteration:  41% 51/125 [00:21<00:31,  2.38it/s]\u001b[A\n",
            "Iteration:  42% 52/125 [00:22<00:30,  2.38it/s]\u001b[A\n",
            "Iteration:  42% 53/125 [00:22<00:30,  2.37it/s]\u001b[A\n",
            "Iteration:  43% 54/125 [00:22<00:29,  2.38it/s]\u001b[A\n",
            "Iteration:  44% 55/125 [00:23<00:29,  2.38it/s]\u001b[A\n",
            "Iteration:  45% 56/125 [00:23<00:29,  2.38it/s]\u001b[A\n",
            "Iteration:  46% 57/125 [00:24<00:28,  2.38it/s]\u001b[A\n",
            "Iteration:  46% 58/125 [00:24<00:28,  2.38it/s]\u001b[A\n",
            "Iteration:  47% 59/125 [00:25<00:27,  2.37it/s]\u001b[A\n",
            "Iteration:  48% 60/125 [00:25<00:27,  2.37it/s]\u001b[A\n",
            "Iteration:  49% 61/125 [00:25<00:26,  2.37it/s]\u001b[A\n",
            "Iteration:  50% 62/125 [00:26<00:26,  2.37it/s]\u001b[A\n",
            "Iteration:  50% 63/125 [00:26<00:26,  2.37it/s]\u001b[A\n",
            "Iteration:  51% 64/125 [00:27<00:25,  2.37it/s]\u001b[A\n",
            "Iteration:  52% 65/125 [00:27<00:25,  2.37it/s]\u001b[A\n",
            "Iteration:  53% 66/125 [00:27<00:24,  2.38it/s]\u001b[A\n",
            "Iteration:  54% 67/125 [00:28<00:24,  2.37it/s]\u001b[A\n",
            "Iteration:  54% 68/125 [00:28<00:24,  2.37it/s]\u001b[A\n",
            "Iteration:  55% 69/125 [00:29<00:23,  2.37it/s]\u001b[A\n",
            "Iteration:  56% 70/125 [00:29<00:23,  2.37it/s]\u001b[A\n",
            "Iteration:  57% 71/125 [00:30<00:22,  2.37it/s]\u001b[A\n",
            "Iteration:  58% 72/125 [00:30<00:22,  2.37it/s]\u001b[A\n",
            "Iteration:  58% 73/125 [00:30<00:21,  2.37it/s]\u001b[A\n",
            "Iteration:  59% 74/125 [00:31<00:21,  2.37it/s]\u001b[A\n",
            "Iteration:  60% 75/125 [00:31<00:21,  2.38it/s]\u001b[A\n",
            "Iteration:  61% 76/125 [00:32<00:20,  2.38it/s]\u001b[A\n",
            "Iteration:  62% 77/125 [00:32<00:20,  2.38it/s]\u001b[A\n",
            "Iteration:  62% 78/125 [00:33<00:19,  2.37it/s]\u001b[A\n",
            "Iteration:  63% 79/125 [00:33<00:19,  2.37it/s]\u001b[A\n",
            "Iteration:  64% 80/125 [00:33<00:18,  2.37it/s]\u001b[A\n",
            "Iteration:  65% 81/125 [00:34<00:18,  2.37it/s]\u001b[A\n",
            "Iteration:  66% 82/125 [00:34<00:18,  2.37it/s]\u001b[A\n",
            "Iteration:  66% 83/125 [00:35<00:17,  2.37it/s]\u001b[A\n",
            "Iteration:  67% 84/125 [00:35<00:17,  2.37it/s]\u001b[A\n",
            "Iteration:  68% 85/125 [00:35<00:16,  2.37it/s]\u001b[A\n",
            "Iteration:  69% 86/125 [00:36<00:16,  2.37it/s]\u001b[A\n",
            "Iteration:  70% 87/125 [00:36<00:16,  2.37it/s]\u001b[A\n",
            "Iteration:  70% 88/125 [00:37<00:15,  2.37it/s]\u001b[A\n",
            "Iteration:  71% 89/125 [00:37<00:15,  2.37it/s]\u001b[A\n",
            "Iteration:  72% 90/125 [00:38<00:14,  2.37it/s]\u001b[A\n",
            "Iteration:  73% 91/125 [00:38<00:14,  2.36it/s]\u001b[A\n",
            "Iteration:  74% 92/125 [00:38<00:13,  2.37it/s]\u001b[A\n",
            "Iteration:  74% 93/125 [00:39<00:13,  2.37it/s]\u001b[A\n",
            "Iteration:  75% 94/125 [00:39<00:13,  2.37it/s]\u001b[A\n",
            "Iteration:  76% 95/125 [00:40<00:12,  2.37it/s]\u001b[A\n",
            "Iteration:  77% 96/125 [00:40<00:12,  2.37it/s]\u001b[A\n",
            "Iteration:  78% 97/125 [00:41<00:11,  2.37it/s]\u001b[A\n",
            "Iteration:  78% 98/125 [00:41<00:11,  2.37it/s]\u001b[A\n",
            "Iteration:  79% 99/125 [00:41<00:10,  2.36it/s]\u001b[A\n",
            "Iteration:  80% 100/125 [00:42<00:10,  2.36it/s]\u001b[A\n",
            "Iteration:  81% 101/125 [00:42<00:10,  2.37it/s]\u001b[A\n",
            "Iteration:  82% 102/125 [00:43<00:09,  2.37it/s]\u001b[A\n",
            "Iteration:  82% 103/125 [00:43<00:09,  2.37it/s]\u001b[A\n",
            "Iteration:  83% 104/125 [00:43<00:08,  2.37it/s]\u001b[A\n",
            "Iteration:  84% 105/125 [00:44<00:08,  2.36it/s]\u001b[A\n",
            "Iteration:  85% 106/125 [00:44<00:08,  2.37it/s]\u001b[A\n",
            "Iteration:  86% 107/125 [00:45<00:07,  2.37it/s]\u001b[A\n",
            "Iteration:  86% 108/125 [00:45<00:07,  2.37it/s]\u001b[A\n",
            "Iteration:  87% 109/125 [00:46<00:06,  2.36it/s]\u001b[A\n",
            "Iteration:  88% 110/125 [00:46<00:06,  2.37it/s]\u001b[A\n",
            "Iteration:  89% 111/125 [00:46<00:05,  2.37it/s]\u001b[A\n",
            "Iteration:  90% 112/125 [00:47<00:05,  2.37it/s]\u001b[A\n",
            "Iteration:  90% 113/125 [00:47<00:05,  2.37it/s]\u001b[A\n",
            "Iteration:  91% 114/125 [00:48<00:04,  2.37it/s]\u001b[A\n",
            "Iteration:  92% 115/125 [00:48<00:04,  2.37it/s]\u001b[A\n",
            "Iteration:  93% 116/125 [00:49<00:03,  2.36it/s]\u001b[A\n",
            "Iteration:  94% 117/125 [00:49<00:03,  2.36it/s]\u001b[A\n",
            "Iteration:  94% 118/125 [00:49<00:02,  2.36it/s]\u001b[A\n",
            "Iteration:  95% 119/125 [00:50<00:02,  2.36it/s]\u001b[A\n",
            "Iteration:  96% 120/125 [00:50<00:02,  2.36it/s]\u001b[A\n",
            "Iteration:  97% 121/125 [00:51<00:01,  2.36it/s]\u001b[A\n",
            "Iteration:  98% 122/125 [00:51<00:01,  2.36it/s]\u001b[A\n",
            "Iteration:  98% 123/125 [00:52<00:00,  2.36it/s]\u001b[A\n",
            "Iteration:  99% 124/125 [00:52<00:00,  2.36it/s]\u001b[A\n",
            "Iteration: 100% 125/125 [00:52<00:00,  2.36it/s]\u001b[A\n",
            "Epoch: 100% 1/1 [00:52<00:00, 52.88s/it]\n",
            "11/18/2019 20:45:19 - INFO - __main__ -    global_step = 125, average loss = 0.661280591726303\n",
            "11/18/2019 20:45:19 - INFO - __main__ -   Saving model checkpoint to ./Bert_Classifier/\n",
            "11/18/2019 20:45:19 - INFO - transformers.configuration_utils -   Configuration saved in ./Bert_Classifier/config.json\n",
            "11/18/2019 20:45:20 - INFO - transformers.modeling_utils -   Model weights saved in ./Bert_Classifier/pytorch_model.bin\n",
            "11/18/2019 20:45:20 - INFO - transformers.configuration_utils -   loading configuration file ./Bert_Classifier/config.json\n",
            "11/18/2019 20:45:20 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/18/2019 20:45:20 - INFO - transformers.modeling_utils -   loading weights file ./Bert_Classifier/pytorch_model.bin\n",
            "11/18/2019 20:45:23 - INFO - transformers.tokenization_utils -   Model name './Bert_Classifier/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './Bert_Classifier/' is a path or url to a directory containing tokenizer files.\n",
            "11/18/2019 20:45:23 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/vocab.txt\n",
            "11/18/2019 20:45:23 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/added_tokens.json\n",
            "11/18/2019 20:45:23 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/special_tokens_map.json\n",
            "11/18/2019 20:45:23 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/tokenizer_config.json\n",
            "11/18/2019 20:45:23 - INFO - transformers.tokenization_utils -   Model name './Bert_Classifier/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './Bert_Classifier/' is a path or url to a directory containing tokenizer files.\n",
            "11/18/2019 20:45:23 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/vocab.txt\n",
            "11/18/2019 20:45:23 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/added_tokens.json\n",
            "11/18/2019 20:45:23 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/special_tokens_map.json\n",
            "11/18/2019 20:45:23 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/tokenizer_config.json\n",
            "11/18/2019 20:45:23 - INFO - __main__ -   Evaluate the following checkpoints: ['./Bert_Classifier/']\n",
            "11/18/2019 20:45:23 - INFO - transformers.configuration_utils -   loading configuration file ./Bert_Classifier/config.json\n",
            "11/18/2019 20:45:23 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/18/2019 20:45:23 - INFO - transformers.modeling_utils -   loading weights file ./Bert_Classifier/pytorch_model.bin\n",
            "11/18/2019 20:45:27 - INFO - __main__ -   Loading features from cached file ./09_BERT/Small_Dataset/cached_dev_bert-base-uncased_128_cola\n",
            "11/18/2019 20:45:27 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/18/2019 20:45:27 - INFO - __main__ -     Num examples = 100\n",
            "11/18/2019 20:45:27 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 13/13 [00:01<00:00,  7.94it/s]\n",
            "11/18/2019 20:45:28 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/18/2019 20:45:28 - INFO - __main__ -     mcc = 0.22935415401872886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1SZ9fLzHrPM",
        "colab_type": "text"
      },
      "source": [
        "If you trained with the small dataset you should see a final result such as mcc = 0.24.\n",
        "\n",
        "On the other hand if you trained with the large one mcc increases to 0.70\n",
        "\n",
        "The MCC score measures how well does the algorithm perform on both positive and negative predictions.\n",
        "\n",
        "It gives more information than the accuracy or the f1 score.\n",
        "\n",
        "This numbers ranges from -1 to 1 being 0 the random case, -1 the worst value and +1 the best value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUmytTFLWSZp",
        "colab_type": "text"
      },
      "source": [
        "## Further Evaluation\n",
        "\n",
        "Let's compute the metrics of our fine-tuned model to see how well it performs on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au85e90b2Ed6",
        "colab_type": "code",
        "outputId": "4df8db27-b27e-4dc2-de6a-a306c5e65bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "preds = np.loadtxt(\"./Bert_Classifier/predictions.txt\")\n",
        "test = pd.read_csv(os.environ[\"DATA_DIR\"] + \"dev.tsv\", header=None, sep = '\\t')\n",
        "\n",
        "print(classification_report(np.asarray(test[1]), preds))\n",
        "print(\"Accuracy: \", accuracy_score(np.asarray(test[1]), preds))\n",
        "print(\"MCC: \", matthews_corrcoef(test[1], preds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.70      0.67        54\n",
            "           1       0.60      0.52      0.56        46\n",
            "\n",
            "    accuracy                           0.62       100\n",
            "   macro avg       0.62      0.61      0.61       100\n",
            "weighted avg       0.62      0.62      0.62       100\n",
            "\n",
            "Accuracy:  0.62\n",
            "MCC:  0.22935415401872886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1l41weoHzUW",
        "colab_type": "text"
      },
      "source": [
        "You should see an accuracy of 62% and an f1-score of 62% which is a good result considering the size of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSGsGLSNFkpr",
        "colab_type": "text"
      },
      "source": [
        "The full model fine-tuned on the 500k tweets achieve the following metrics:\n",
        "\n",
        "    - Accuracy = 0.85\n",
        "    - Recall = 0.85\n",
        "    - Precision = 0.86\n",
        "    - Recall = 0.85\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yIjsNITWfGg",
        "colab_type": "text"
      },
      "source": [
        "## Perform inference\n",
        "\n",
        "Now let's take some random examples from our test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zyHDRIKBSeN",
        "colab_type": "code",
        "outputId": "76cf7085-9d8d-4e27-e03b-18c1072558c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "os.mkdir(\"./Test_Dataset/\") # We are going to store the test dataset in this folder\n",
        "\n",
        "test_evaluate = test[:4]\n",
        "print(test_evaluate)\n",
        "test_evaluate.to_csv(\"./Test_Dataset/dev.tsv\", sep='\\t', index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0  1  2                                                  3\n",
            "0  0  1  a  Now Playing: ♬ Dick Curless - Evil Hearted Me ...\n",
            "1  1  0  a  Not only are you comfortably swaddled in secur...\n",
            "2  2  1  a  Follow @iAmMySign !!!  Follow @iAmMySign our o...\n",
            "3  3  0  a  These strawberry sandwich cookies are so easy ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry28Kp1oIZgR",
        "colab_type": "text"
      },
      "source": [
        "If you want to perform inference with the larger model we provide an already trained version. You only have to change the argument in model_name_or path from Bert_Classifier_small to Bert_Classifier_Large"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQHEysl0QQWq",
        "colab_type": "code",
        "outputId": "fd5b4f2f-3add-423d-9a50-83cdfef45565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%env MODEL_PATH=./Bert_Classifier/\n",
        "\n",
        "#Uncomment the following line to use the version of the model trained with the large dataset\n",
        "#%env MODEL_PATH=./09_BERT/Bert_Classifier_Large/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: MODEL_PATH=./Bert_Classifier/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWOTIq74DqOj",
        "colab_type": "code",
        "outputId": "b5f57f0f-0504-4aac-9336-85babb34d159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ./09_BERT/run_glue.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path \"$MODEL_PATH\" \\\n",
        "    --task_name CoLA \\\n",
        "    --do_eval \\\n",
        "    --do_lower_case \\\n",
        "    --data_dir ./Test_Dataset/ \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_eval_batch_size=8   \\\n",
        "    --per_gpu_train_batch_size=8   \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 1.0 \\\n",
        "    --save_steps 62500 \\\n",
        "    --output_dir  \"$MODEL_PATH\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/18/2019 20:49:24 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/18/2019 20:49:24 - INFO - transformers.configuration_utils -   loading configuration file ./Bert_Classifier/config.json\n",
            "11/18/2019 20:49:24 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/18/2019 20:49:24 - INFO - transformers.tokenization_utils -   Model name './Bert_Classifier/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './Bert_Classifier/' is a path or url to a directory containing tokenizer files.\n",
            "11/18/2019 20:49:24 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/vocab.txt\n",
            "11/18/2019 20:49:24 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/added_tokens.json\n",
            "11/18/2019 20:49:24 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/special_tokens_map.json\n",
            "11/18/2019 20:49:24 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/tokenizer_config.json\n",
            "11/18/2019 20:49:24 - INFO - transformers.modeling_utils -   loading weights file ./Bert_Classifier/pytorch_model.bin\n",
            "11/18/2019 20:49:30 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='./Test_Dataset/', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./Bert_Classifier/', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='./Bert_Classifier/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=62500, seed=42, server_ip='', server_port='', task_name='cola', tokenizer_name='', tpu=False, tpu_ip_address='', tpu_name='', warmup_steps=0, weight_decay=0.0, xrt_tpu_config='')\n",
            "11/18/2019 20:49:30 - INFO - transformers.tokenization_utils -   Model name './Bert_Classifier/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './Bert_Classifier/' is a path or url to a directory containing tokenizer files.\n",
            "11/18/2019 20:49:30 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/vocab.txt\n",
            "11/18/2019 20:49:30 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/added_tokens.json\n",
            "11/18/2019 20:49:30 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/special_tokens_map.json\n",
            "11/18/2019 20:49:30 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/tokenizer_config.json\n",
            "11/18/2019 20:49:30 - INFO - __main__ -   Evaluate the following checkpoints: ['./Bert_Classifier/']\n",
            "11/18/2019 20:49:30 - INFO - transformers.configuration_utils -   loading configuration file ./Bert_Classifier/config.json\n",
            "11/18/2019 20:49:30 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/18/2019 20:49:30 - INFO - transformers.modeling_utils -   loading weights file ./Bert_Classifier/pytorch_model.bin\n",
            "11/18/2019 20:49:33 - INFO - __main__ -   Creating features from dataset file at ./Test_Dataset/\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   input_ids: 101 2085 2652 1024 100 5980 15390 7971 1011 4763 18627 2033 100 16770 1024 1013 1013 1056 1012 2522 1013 1042 2480 21600 2683 4313 2102 2475 2232 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   input_ids: 101 2025 2069 2024 2017 18579 25430 4215 20043 1999 3036 2651 1010 2009 1521 1055 1012 1012 1012 2062 2005 6178 7277 9691 16770 1024 1013 1013 1056 1012 2522 1013 19842 15721 2072 2549 2290 2487 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   input_ids: 101 3582 1030 24264 18879 5332 16206 999 999 999 3582 1030 24264 18879 5332 16206 2256 2880 3931 2005 1996 2878 28501 1012 3582 1030 24264 18879 5332 16206 999 999 999 3582 1030 24264 18879 5332 16206 999 999 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   input_ids: 101 2122 16876 11642 16324 2024 2061 3733 2000 2191 1998 2061 11937 21756 999 3819 2005 1001 10756 10259 16770 1024 1013 1013 1056 1012 2522 1013 1057 4160 2581 3597 2953 2475 2100 2581 3081 1030 24264 20492 28433 8159 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/18/2019 20:49:33 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/18/2019 20:49:33 - INFO - __main__ -   Saving features into cached file ./Test_Dataset/cached_dev_Bert_Classifier_128_cola\n",
            "11/18/2019 20:49:33 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/18/2019 20:49:33 - INFO - __main__ -     Num examples = 4\n",
            "11/18/2019 20:49:33 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 1/1 [00:00<00:00, 11.22it/s]\n",
            "11/18/2019 20:49:33 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/18/2019 20:49:33 - INFO - __main__ -     mcc = -1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC6oati6H-cx",
        "colab_type": "text"
      },
      "source": [
        "mcc\n",
        "\n",
        "Let's see if the model has correctly classified the examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7chkHgKMECc1",
        "colab_type": "code",
        "outputId": "76c2e04b-43c7-4bf7-d2df-4e7a805a4606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import os\n",
        "\n",
        "results = np.loadtxt(os.environ['MODEL_PATH'] + \"predictions.txt\")\n",
        "for i,t in enumerate(test_evaluate[3]):\n",
        "    print(t + \" --> \", \"BOT\" if results[i]> 0.5 else \"NOT A BOT\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now Playing: ♬ Dick Curless - Evil Hearted Me ♬ https://t.co/fzgP9IRt2h -->  NOT A BOT\n",
            "Not only are you comfortably swaddled in security today, it’s ... More for Capricorn https://t.co/MVCHEli4g1 -->  BOT\n",
            "Follow @iAmMySign !!!  Follow @iAmMySign our official page for the whole Zodiac.  Follow @iAmMySign !!!  Follow @iAmMySign !!! -->  NOT A BOT\n",
            "These strawberry sandwich cookies are so easy to make and so tasty! Perfect for #MothersDay https://t.co/Uq7cooR2y7 via @iamthemaven -->  BOT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUX8iM5tFZvn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE6tv1TEzYsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}