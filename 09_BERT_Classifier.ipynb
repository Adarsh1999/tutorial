{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"09_BERT_Classifier.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Y3VSgZXfFdCk","colab_type":"text"},"source":["# Fine tuning pre-trained language models for text classification\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yzrX_0SDViOu","colab_type":"text"},"source":["## Overview\n","Fine-tuning pre-trained language models learnt with transformers has improved the state of the art in multiple NLP evaluation tasks(see [SuperGlue leader board](https://super.gluebenchmark.com/leaderboard)). Learning a language model is an unsupervised task where the model learns to predict the next word in a sequence given the previous words. Neural language models have been implemented as feed foorward networks, LSTMS (ELMo, ULMFit), and transformers-encoders (BERT) or decoders (Open AI GPT).\n","\n","In this notebook we fine tune a BERT pre-trained language model to carry out a binary classification task where tweets are labelled as generated by bots or hurmans. \n","\n","<!-- The notebook is structured as follows:\n","\n","- Motivation\n","- Setup\n","  - Libraries required\n","  - Dataset\n","- A glimpse on BERT tokenization \n","- Fine tune the model\n","- Evaluate the BERT classifier \n","-->"]},{"cell_type":"markdown","metadata":{"id":"QUlcUwnALTCn","colab_type":"text"},"source":["## Motivation\n","While word embeddings are learnt from large corpora, their use in neural models to solve specific tasks is limited to the input layer. So in practice a task-specific neural model is built almost from scratch because most of the model parameters are initialized randomly, and hence, these paremeters need to be optimized for the task at hand, requiring large sets of data to produce a high performance model.\n","\n","Recent advances in neural language models (BERT or OPEN AI GPT) have shown evidence that task specific architectures are not longer necessary and transfering some internal representations (attention blocks) along with shallow feed forward networks is enough. \n","\n","In (Garcia et al.,2019) we presented an experimental study on the use of word embeddings as input of CNN architectures and Bi-LSTM to tackle the bot detection task and compare these results with fine-tuning pretrained language models. \n","\n","Evaluation results, presented in the figure below, show that fine-tuning language models yields overall better results than training specific neural architectures that are fed with mixture of: i) pre-trained contextualized word embeddings (ELMo), ii) pre-trained  context-indepedent word embeddings learnt from Common Crawl(FastText), Twitter (GloVe), and urban dictionary (word2vec), plus embeddings optimized by the neural network in the learning process. \n","\n","![Bot detection classification task](https://drive.google.com/uc?id=1rSzM544MK2QOezpvUKHfrxATbkEiyBHX)\n","\n","\n","\n","**References**\n","\n","Garcia-Silva, Andres, et al. \"An Empirical Study on Pre-trained Embeddings and Language Models for Bot Detection.\" Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019). 2019.\n","\n","To cite this paper use the following BibTex entry: \n","\n","```\n","@inproceedings{garcia-silva-etal-2019-empirical,\n","    title = \"An Empirical Study on Pre-trained Embeddings and Language Models for Bot Detection\",\n","    author = \"Garcia-Silva, Andres  and\n","      Berrio, Cristian  and\n","      G{\\'o}mez-P{\\'e}rez, Jos{\\'e} Manuel\",\n","    booktitle = \"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)\",\n","    month = aug,\n","    year = \"2019\",\n","    address = \"Florence, Italy\",\n","    publisher = \"Association for Computational Linguistics\",\n","    url = \"https://www.aclweb.org/anthology/W19-4317\",\n","    doi = \"10.18653/v1/W19-4317\",\n","    pages = \"148--155\",\n","}\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"POOq3acQMrKT","colab_type":"text"},"source":["# A glimpse on BERT\n"]},{"cell_type":"markdown","metadata":{"id":"1U6jzPnupzRr","colab_type":"text"},"source":["## Input representation\n","\n","<img src=\"https://drive.google.com/uc?id=1tZS7sszhNtT3m25EZJkjC9PjRZDEPKGy\" alt=\"Token embeddings, segment embeddings, and positional embeddings\" width=\"500\"/>\n","\n","Image source: \"Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\""]},{"cell_type":"markdown","metadata":{"id":"ta8RVpIpUiyf","colab_type":"text"},"source":["## Pre-training learning objectives"]},{"cell_type":"markdown","metadata":{"id":"hU-qjZNfODqh","colab_type":"text"},"source":["### Masked language model\n","\n","<img src=\"https://drive.google.com/uc?id=11GJiHlDeoKsShOwSJvMTq3fc8E8GxV4V\" alt=\"Masked LM\" width=\"500\"/>\n","\n","Image Source: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"]},{"cell_type":"markdown","metadata":{"id":"yPqLDsHCQh6U","colab_type":"text"},"source":["### Next Sentence Prediction\n","\n","<img src=\"https://drive.google.com/uc?id=1x0ckvgMwb5j3SfVNId-EyQgyDlt3C42h\" alt=\"next sentence prediction\" width=\"500\"/>\n","\n","Image source: http://jalammar.github.io/illustrated-bert/\n"]},{"cell_type":"markdown","metadata":{"id":"3Pt2r_VzV1AS","colab_type":"text"},"source":["## Contextualized word embeddings\n","\n","<img src=\"https://drive.google.com/uc?id=1LozTCltkxXbkrE2M72r0lDUWEDXLEWZU\" alt=\"Contextualized embeddings\" width=\"600\"/>\n","\n","Image source: http://jalammar.github.io/illustrated-bert/\n"]},{"cell_type":"markdown","metadata":{"id":"ZtN3rqyOOEbB","colab_type":"text"},"source":["## Fine-tuning\n","\n","fine-tuning BERT for a task just requires to incorporate one  additional  output  layer,  so a minimal number of parameters need to be learned from scratch.\n","\n","In the figure below E represents the input embedding,Ti represents the contextual representation of token i, [CLS] is the special symbol for classification output, and [SEP] is the specialsymbol to separate non-consecutive token sequences\n","\n","<img src=\"https://drive.google.com/uc?id=1wZPwbMNtHwf8g-7phWxwtJCxnTfPj-Ux\" width=\"600\"/>\n","\n","Image source: Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n","\n","To **fine-tune BERT for a sequence classification task** the transfomer output for the CLS token is used as the sequence representation. The transfomer output for the CLS token is connected to a one layer feed forward network that predicts the classification labels. All the BERT parameters and the FF network are fine-tune jointly to maximize the log-probability of the correct label.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9HR3iIvgVmnO","colab_type":"text"},"source":["\n","# Experimental setup\n"]},{"cell_type":"markdown","metadata":{"id":"8TZEolAIcM9_","colab_type":"text"},"source":["## Transformers library\n","\n","We use transformer from huggingface: https://github.com/huggingface/transformers\n","\n","\"Transformers provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.\""]},{"cell_type":"code","metadata":{"id":"kGBVqmkNqD_z","colab_type":"code","outputId":"fa7b0082-eb3e-48dd-dcb7-5cd7f22e34b7","executionInfo":{"status":"ok","timestamp":1573719706413,"user_tz":-60,"elapsed":10141,"user":{"displayName":"Expert System - Lab","photoUrl":"","userId":"16197418968100245121"}},"colab":{"base_uri":"https://localhost:8080/","height":612}},"source":["!pip install transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n","\r\u001b[K     |█                               | 10kB 28.4MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n","\u001b[K     |████████████████████████████████| 860kB 44.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 33.3MB/s \n","\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.14)\n","Collecting regex\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n","\u001b[K     |████████████████████████████████| 645kB 41.1MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n","Requirement already satisfied: botocore<1.14.0,>=1.13.14 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.14)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n","Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->transformers) (2.6.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->transformers) (0.15.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=c1a8255c933b6acd9990e2eb3874ada7daf1996813060c31ad6e1d6f04a91ea0\n","  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, regex, transformers\n","Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.1.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jAB8VCbxkjr4","colab_type":"text"},"source":["## Dataset \n","\n","We use the bot detection dataset generated in (Garcia et al.,2019) that was built starting from an existing list of twitter accounts that were manually labelled as bots and humans. Then we use the twitter API to extract tweets from these account.  In total the dataset contains around 600K tweet, approximately half of them generated by bots, and the other half by humans. \n","\n","In this notebook we provide a complete version of the dataset (large) and a reduced one (small) to be able to run the notebook whithin the time frame, since **fine tuning BERT on the large version takes more than 5h**. \n","\n","- Large: 500k train and 100k test labeled tweets which is in the path: \"'/content/gdrive/My Drive/09_BERT/Large_Dataset/\"\n","- Small: 1k train and 100 test labeled tweets which is in the path: \"'/content/gdrive/My Drive/09_BERT/Small_Dataset/\""]},{"cell_type":"markdown","metadata":{"id":"XViapGYvR01H","colab_type":"text"},"source":["### Mounting Google Drive\n","\n","Let's access the google drive where we have stored the datasets and the pretrained models\n","\n","When running the following cell a link will appear, following that link you will find an authorization code to be pasted below.\n","This is necessary to access the Google Drive account."]},{"cell_type":"code","metadata":{"id":"oHMrR1A0s4q4","colab_type":"code","outputId":"4afcc9ff-45d9-4355-afc5-22c03b529783","executionInfo":{"status":"ok","timestamp":1573719668956,"user_tz":-60,"elapsed":24559,"user":{"displayName":"Expert System - Lab","photoUrl":"","userId":"16197418968100245121"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2Va-9Z80pAQr","colab_type":"text"},"source":["### Set the dataset version\n","The enviroment variable DATA_DIR holds the path to the dataset "]},{"cell_type":"code","metadata":{"id":"v-Bg5I-VrylZ","colab_type":"code","outputId":"6c61effc-8293-4a91-b88b-c5bd53b37c90","executionInfo":{"status":"ok","timestamp":1573732430781,"user_tz":-60,"elapsed":974,"user":{"displayName":"Expert System - Lab","photoUrl":"","userId":"16197418968100245121"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%env DATA_DIR=/content/gdrive/My Drive/09_BERT/Small_Dataset/\n","\n","#Uncomment the following line to use the large version of the dataset\n","#%env DATA_DIR=/content/gdrive/My Drive/09_BERT/Large_Dataset/"],"execution_count":25,"outputs":[{"output_type":"stream","text":["env: DATA_DIR=/content/gdrive/My Drive/09_BERT/Small_Dataset/\n","env: DATA_DIR=/content/gdrive/My Drive/09_BERT/Large_Dataset/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-8WmMvszp3so","colab_type":"text"},"source":["### Inspect the dataset\n","\n","The dataset is in the tsv format expected by transfomer library. "]},{"cell_type":"code","metadata":{"id":"zymDOvKEcMi5","colab_type":"code","outputId":"f4a05369-1707-4e4b-8606-6f9dbcb7e447","executionInfo":{"status":"ok","timestamp":1573731912029,"user_tz":-60,"elapsed":2269,"user":{"displayName":"Expert System - Lab","photoUrl":"","userId":"16197418968100245121"}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["import os\n","import pandas as pd\n","\n","test = pd.read_csv(os.environ[\"DATA_DIR\"] + \"dev.tsv\", header=None, sep = '\\t')\n","data = pd.DataFrame(test)\n","data.columns = [\"index\", \"label\", \"mark\", \"tweet\"]\n","data"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>label</th>\n","      <th>mark</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>a</td>\n","      <td>Now Playing: ♬ Dick Curless - Evil Hearted Me ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>a</td>\n","      <td>Not only are you comfortably swaddled in secur...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>a</td>\n","      <td>Follow @iAmMySign !!!  Follow @iAmMySign our o...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>a</td>\n","      <td>These strawberry sandwich cookies are so easy ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>a</td>\n","      <td>Do These Two Lines Match Up On Your Hands Here...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>99995</th>\n","      <td>99995</td>\n","      <td>0</td>\n","      <td>a</td>\n","      <td>I Love The World Cup !!</td>\n","    </tr>\n","    <tr>\n","      <th>99996</th>\n","      <td>99996</td>\n","      <td>0</td>\n","      <td>a</td>\n","      <td>CONFIRMED: Today is it for the season for one ...</td>\n","    </tr>\n","    <tr>\n","      <th>99997</th>\n","      <td>99997</td>\n","      <td>0</td>\n","      <td>a</td>\n","      <td>Stack that paper up &amp;amp; then make boss moves...</td>\n","    </tr>\n","    <tr>\n","      <th>99998</th>\n","      <td>99998</td>\n","      <td>1</td>\n","      <td>a</td>\n","      <td>Wednesday Free Pick is Live! #freepicks #freet...</td>\n","    </tr>\n","    <tr>\n","      <th>99999</th>\n","      <td>99999</td>\n","      <td>1</td>\n","      <td>a</td>\n","      <td>Download Monster Hunter Generations Ultimate t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100000 rows × 4 columns</p>\n","</div>"],"text/plain":["       index  label mark                                              tweet\n","0          0      1    a  Now Playing: ♬ Dick Curless - Evil Hearted Me ...\n","1          1      0    a  Not only are you comfortably swaddled in secur...\n","2          2      1    a  Follow @iAmMySign !!!  Follow @iAmMySign our o...\n","3          3      0    a  These strawberry sandwich cookies are so easy ...\n","4          4      0    a  Do These Two Lines Match Up On Your Hands Here...\n","...      ...    ...  ...                                                ...\n","99995  99995      0    a                            I Love The World Cup !!\n","99996  99996      0    a  CONFIRMED: Today is it for the season for one ...\n","99997  99997      0    a  Stack that paper up &amp; then make boss moves...\n","99998  99998      1    a  Wednesday Free Pick is Live! #freepicks #freet...\n","99999  99999      1    a  Download Monster Hunter Generations Ultimate t...\n","\n","[100000 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"_pOZytkUVDJt","colab_type":"text"},"source":["# Hands-on"]},{"cell_type":"markdown","metadata":{"id":"NVRF_BMtOFBK","colab_type":"text"},"source":["## Tokenization \n","\n","Recent neural languages models use subword representations. ELMO relies on characters, Open AI GPT on byte pair encoding, and BERT on the word pieces algorithms. These **subword representations are combined when unseen words during training needs to be processed, hence avoiding the OOV problem**. \n","\n","BERT uses a 30k WordPieces vocabulary. \n","\n","Let us see how the BERT Tokenizer works"]},{"cell_type":"code","metadata":{"id":"W1Hb2SAeqKrE","colab_type":"code","outputId":"12351ff9-ee5b-4d19-9363-bb7560a82166","executionInfo":{"status":"ok","timestamp":1573721704413,"user_tz":-60,"elapsed":8806,"user":{"displayName":"Expert System - Lab","photoUrl":"","userId":"16197418968100245121"}},"colab":{"base_uri":"https://localhost:8080/","height":114}},"source":["from transformers import *\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","text = input(\"Enter a word or a sentence: \")\n","print(tokenizer.tokenize(text))\n","print(tokenizer.encode(text))\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Enter a word or a sentence: ecent neural languages models use subword representations. ELMO relies on\n","['ec', '##ent', 'neural', 'languages', 'models', 'use', 'sub', '##word', 'representations', '.', 'elm', '##o', 'relies', 'on']\n","[14925, 4765, 15756, 4155, 4275, 2224, 4942, 18351, 15066, 1012, 17709, 2080, 16803, 2006]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"76AcSBFqFyXi","colab_type":"text"},"source":["## Fine-Tuning the model\n","\n","The next step would be to fine-tune the model.\n","\n","Running the following script you can fine-tune the model and perform evaluation. While doing the evaluation the classification of the tweets on the test set is saved in the predictions.txt file that we will use later.\n","\n","The most relevant parameters of the script are:\n","  - model type: the model that we are going to use, in this case BERT\n","  - model name or path: the name of the model or path storing a specific model.\n","  - task name: the task that we want to perform, in this case CoLA because we want to do classification.\n","  - ouput dir: the directory in which it stores the fine-tuned model.\n","  \n","You can try to change the parameters and see how it affects performance. \n","\n","This process is slow even though we reduced the dataset. You should expect that it takes around 1 minute."]},{"cell_type":"code","metadata":{"id":"BE6sLu94qSvR","colab_type":"code","outputId":"91df5220-ca1d-4c8a-bb3b-e82304ab3dfb","executionInfo":{"status":"ok","timestamp":1573722306665,"user_tz":-60,"elapsed":46092,"user":{"displayName":"Expert System - Lab","photoUrl":"","userId":"16197418968100245121"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python /content/gdrive/My\\ Drive/09_BERT/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name CoLA \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_lower_case \\\n","    --data_dir \"$DATA_DIR\" \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=8   \\\n","    --per_gpu_train_batch_size=8   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1.0 \\\n","    --save_steps 62500 \\\n","    --overwrite_output_dir \\\n","    --output_dir  ./Bert_Classifier/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["11/14/2019 09:04:24 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","11/14/2019 09:04:25 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n","11/14/2019 09:04:25 - INFO - transformers.configuration_utils -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"cola\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n","11/14/2019 09:04:26 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","11/14/2019 09:04:27 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","11/14/2019 09:04:30 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","11/14/2019 09:04:30 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","11/14/2019 09:04:34 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/content/gdrive/My Drive/09_BERT/Small_Dataset/', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='./Bert_Classifier/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=62500, seed=42, server_ip='', server_port='', task_name='cola', tokenizer_name='', tpu=False, tpu_ip_address='', tpu_name='', warmup_steps=0, weight_decay=0.0, xrt_tpu_config='')\n","11/14/2019 09:04:34 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/09_BERT/Small_Dataset/cached_train_bert-base-uncased_128_cola\n","11/14/2019 09:04:34 - INFO - __main__ -   ***** Running training *****\n","11/14/2019 09:04:34 - INFO - __main__ -     Num examples = 1000\n","11/14/2019 09:04:34 - INFO - __main__ -     Num Epochs = 1\n","11/14/2019 09:04:34 - INFO - __main__ -     Instantaneous batch size per GPU = 8\n","11/14/2019 09:04:34 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 8\n","11/14/2019 09:04:34 - INFO - __main__ -     Gradient Accumulation steps = 1\n","11/14/2019 09:04:34 - INFO - __main__ -     Total optimization steps = 125\n","Epoch:   0% 0/1 [00:00<?, ?it/s]\n","Iteration:   0% 0/125 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/125 [00:00<00:26,  4.63it/s]\u001b[A\n","Iteration:   2% 2/125 [00:00<00:25,  4.80it/s]\u001b[A\n","Iteration:   2% 3/125 [00:00<00:24,  5.01it/s]\u001b[A\n","Iteration:   3% 4/125 [00:00<00:23,  5.14it/s]\u001b[A\n","Iteration:   4% 5/125 [00:00<00:22,  5.27it/s]\u001b[A\n","Iteration:   5% 6/125 [00:01<00:22,  5.21it/s]\u001b[A\n","Iteration:   6% 7/125 [00:01<00:21,  5.45it/s]\u001b[A\n","Iteration:   6% 8/125 [00:01<00:20,  5.63it/s]\u001b[A\n","Iteration:   7% 9/125 [00:01<00:19,  5.82it/s]\u001b[A\n","Iteration:   8% 10/125 [00:01<00:19,  5.97it/s]\u001b[A\n","Iteration:   9% 11/125 [00:01<00:18,  6.07it/s]\u001b[A\n","Iteration:  10% 12/125 [00:02<00:18,  6.08it/s]\u001b[A\n","Iteration:  10% 13/125 [00:02<00:18,  6.07it/s]\u001b[A\n","Iteration:  11% 14/125 [00:02<00:18,  6.08it/s]\u001b[A\n","Iteration:  12% 15/125 [00:02<00:18,  6.04it/s]\u001b[A\n","Iteration:  13% 16/125 [00:02<00:18,  5.99it/s]\u001b[A\n","Iteration:  14% 17/125 [00:02<00:17,  6.04it/s]\u001b[A\n","Iteration:  14% 18/125 [00:03<00:17,  6.03it/s]\u001b[A\n","Iteration:  15% 19/125 [00:03<00:17,  5.92it/s]\u001b[A\n","Iteration:  16% 20/125 [00:03<00:17,  5.99it/s]\u001b[A\n","Iteration:  17% 21/125 [00:03<00:17,  6.05it/s]\u001b[A\n","Iteration:  18% 22/125 [00:03<00:16,  6.06it/s]\u001b[A\n","Iteration:  18% 23/125 [00:03<00:16,  6.09it/s]\u001b[A\n","Iteration:  19% 24/125 [00:04<00:16,  6.01it/s]\u001b[A\n","Iteration:  20% 25/125 [00:04<00:16,  6.01it/s]\u001b[A\n","Iteration:  21% 26/125 [00:04<00:16,  6.08it/s]\u001b[A\n","Iteration:  22% 27/125 [00:04<00:16,  6.05it/s]\u001b[A\n","Iteration:  22% 28/125 [00:04<00:15,  6.07it/s]\u001b[A\n","Iteration:  23% 29/125 [00:04<00:15,  6.11it/s]\u001b[A\n","Iteration:  24% 30/125 [00:05<00:16,  5.89it/s]\u001b[A\n","Iteration:  25% 31/125 [00:05<00:15,  5.96it/s]\u001b[A\n","Iteration:  26% 32/125 [00:05<00:15,  6.05it/s]\u001b[A\n","Iteration:  26% 33/125 [00:05<00:15,  6.12it/s]\u001b[A\n","Iteration:  27% 34/125 [00:05<00:14,  6.17it/s]\u001b[A\n","Iteration:  28% 35/125 [00:05<00:14,  6.15it/s]\u001b[A\n","Iteration:  29% 36/125 [00:06<00:14,  6.19it/s]\u001b[A\n","Iteration:  30% 37/125 [00:06<00:14,  6.00it/s]\u001b[A\n","Iteration:  30% 38/125 [00:06<00:14,  5.87it/s]\u001b[A\n","Iteration:  31% 39/125 [00:06<00:15,  5.72it/s]\u001b[A\n","Iteration:  32% 40/125 [00:06<00:14,  5.69it/s]\u001b[A\n","Iteration:  33% 41/125 [00:06<00:14,  5.67it/s]\u001b[A\n","Iteration:  34% 42/125 [00:07<00:14,  5.67it/s]\u001b[A\n","Iteration:  34% 43/125 [00:07<00:14,  5.77it/s]\u001b[A\n","Iteration:  35% 44/125 [00:07<00:14,  5.68it/s]\u001b[A\n","Iteration:  36% 45/125 [00:07<00:14,  5.64it/s]\u001b[A\n","Iteration:  37% 46/125 [00:07<00:13,  5.65it/s]\u001b[A\n","Iteration:  38% 47/125 [00:08<00:14,  5.55it/s]\u001b[A\n","Iteration:  38% 48/125 [00:08<00:13,  5.53it/s]\u001b[A\n","Iteration:  39% 49/125 [00:08<00:13,  5.47it/s]\u001b[A\n","Iteration:  40% 50/125 [00:08<00:13,  5.49it/s]\u001b[A\n","Iteration:  41% 51/125 [00:08<00:13,  5.51it/s]\u001b[A\n","Iteration:  42% 52/125 [00:08<00:13,  5.56it/s]\u001b[A\n","Iteration:  42% 53/125 [00:09<00:12,  5.58it/s]\u001b[A\n","Iteration:  43% 54/125 [00:09<00:12,  5.59it/s]\u001b[A\n","Iteration:  44% 55/125 [00:09<00:12,  5.75it/s]\u001b[A\n","Iteration:  45% 56/125 [00:09<00:12,  5.60it/s]\u001b[A\n","Iteration:  46% 57/125 [00:09<00:12,  5.59it/s]\u001b[A\n","Iteration:  46% 58/125 [00:10<00:12,  5.58it/s]\u001b[A\n","Iteration:  47% 59/125 [00:10<00:11,  5.58it/s]\u001b[A\n","Iteration:  48% 60/125 [00:10<00:11,  5.50it/s]\u001b[A\n","Iteration:  49% 61/125 [00:10<00:11,  5.54it/s]\u001b[A\n","Iteration:  50% 62/125 [00:10<00:11,  5.55it/s]\u001b[A\n","Iteration:  50% 63/125 [00:10<00:11,  5.51it/s]\u001b[A\n","Iteration:  51% 64/125 [00:11<00:11,  5.51it/s]\u001b[A\n","Iteration:  52% 65/125 [00:11<00:10,  5.63it/s]\u001b[A\n","Iteration:  53% 66/125 [00:11<00:10,  5.79it/s]\u001b[A\n","Iteration:  54% 67/125 [00:11<00:09,  5.88it/s]\u001b[A\n","Iteration:  54% 68/125 [00:11<00:09,  5.94it/s]\u001b[A\n","Iteration:  55% 69/125 [00:11<00:09,  5.94it/s]\u001b[A\n","Iteration:  56% 70/125 [00:12<00:09,  5.97it/s]\u001b[A\n","Iteration:  57% 71/125 [00:12<00:09,  5.92it/s]\u001b[A\n","Iteration:  58% 72/125 [00:12<00:08,  6.00it/s]\u001b[A\n","Iteration:  58% 73/125 [00:12<00:08,  6.03it/s]\u001b[A\n","Iteration:  59% 74/125 [00:12<00:08,  5.91it/s]\u001b[A\n","Iteration:  60% 75/125 [00:12<00:08,  5.79it/s]\u001b[A\n","Iteration:  61% 76/125 [00:13<00:08,  5.69it/s]\u001b[A\n","Iteration:  62% 77/125 [00:13<00:08,  5.76it/s]\u001b[A\n","Iteration:  62% 78/125 [00:13<00:08,  5.85it/s]\u001b[A\n","Iteration:  63% 79/125 [00:13<00:07,  5.96it/s]\u001b[A\n","Iteration:  64% 80/125 [00:13<00:07,  5.81it/s]\u001b[A\n","Iteration:  65% 81/125 [00:13<00:07,  5.75it/s]\u001b[A\n","Iteration:  66% 82/125 [00:14<00:07,  5.72it/s]\u001b[A\n","Iteration:  66% 83/125 [00:14<00:07,  5.83it/s]\u001b[A\n","Iteration:  67% 84/125 [00:14<00:06,  5.86it/s]\u001b[A\n","Iteration:  68% 85/125 [00:14<00:06,  5.97it/s]\u001b[A\n","Iteration:  69% 86/125 [00:14<00:06,  5.88it/s]\u001b[A\n","Iteration:  70% 87/125 [00:15<00:06,  5.76it/s]\u001b[A\n","Iteration:  70% 88/125 [00:15<00:06,  5.67it/s]\u001b[A\n","Iteration:  71% 89/125 [00:15<00:06,  5.82it/s]\u001b[A\n","Iteration:  72% 90/125 [00:15<00:06,  5.77it/s]\u001b[A\n","Iteration:  73% 91/125 [00:15<00:05,  5.80it/s]\u001b[A\n","Iteration:  74% 92/125 [00:15<00:05,  5.91it/s]\u001b[A\n","Iteration:  74% 93/125 [00:16<00:05,  6.00it/s]\u001b[A\n","Iteration:  75% 94/125 [00:16<00:05,  5.91it/s]\u001b[A\n","Iteration:  76% 95/125 [00:16<00:05,  5.83it/s]\u001b[A\n","Iteration:  77% 96/125 [00:16<00:05,  5.77it/s]\u001b[A\n","Iteration:  78% 97/125 [00:16<00:04,  5.66it/s]\u001b[A\n","Iteration:  78% 98/125 [00:16<00:04,  5.56it/s]\u001b[A\n","Iteration:  79% 99/125 [00:17<00:04,  5.52it/s]\u001b[A\n","Iteration:  80% 100/125 [00:17<00:04,  5.40it/s]\u001b[A\n","Iteration:  81% 101/125 [00:17<00:04,  5.30it/s]\u001b[A\n","Iteration:  82% 102/125 [00:17<00:04,  5.36it/s]\u001b[A\n","Iteration:  82% 103/125 [00:17<00:04,  5.43it/s]\u001b[A\n","Iteration:  83% 104/125 [00:18<00:03,  5.40it/s]\u001b[A\n","Iteration:  84% 105/125 [00:18<00:03,  5.46it/s]\u001b[A\n","Iteration:  85% 106/125 [00:18<00:03,  5.61it/s]\u001b[A\n","Iteration:  86% 107/125 [00:18<00:03,  5.76it/s]\u001b[A\n","Iteration:  86% 108/125 [00:18<00:02,  5.89it/s]\u001b[A\n","Iteration:  87% 109/125 [00:18<00:02,  5.95it/s]\u001b[A\n","Iteration:  88% 110/125 [00:19<00:02,  5.99it/s]\u001b[A\n","Iteration:  89% 111/125 [00:19<00:02,  5.79it/s]\u001b[A\n","Iteration:  90% 112/125 [00:19<00:02,  5.90it/s]\u001b[A\n","Iteration:  90% 113/125 [00:19<00:02,  5.82it/s]\u001b[A\n","Iteration:  91% 114/125 [00:19<00:01,  5.83it/s]\u001b[A\n","Iteration:  92% 115/125 [00:19<00:01,  5.92it/s]\u001b[A\n","Iteration:  93% 116/125 [00:20<00:01,  5.96it/s]\u001b[A\n","Iteration:  94% 117/125 [00:20<00:01,  6.01it/s]\u001b[A\n","Iteration:  94% 118/125 [00:20<00:01,  6.05it/s]\u001b[A\n","Iteration:  95% 119/125 [00:20<00:01,  5.92it/s]\u001b[A\n","Iteration:  96% 120/125 [00:20<00:00,  5.98it/s]\u001b[A\n","Iteration:  97% 121/125 [00:20<00:00,  5.86it/s]\u001b[A\n","Iteration:  98% 122/125 [00:21<00:00,  5.68it/s]\u001b[A\n","Iteration:  98% 123/125 [00:21<00:00,  5.79it/s]\u001b[A\n","Iteration:  99% 124/125 [00:21<00:00,  5.91it/s]\u001b[A\n","Iteration: 100% 125/125 [00:21<00:00,  5.86it/s]\u001b[A\n","Epoch: 100% 1/1 [00:21<00:00, 21.61s/it]\n","11/14/2019 09:04:56 - INFO - __main__ -    global_step = 125, average loss = 0.6756048345565796\n","11/14/2019 09:04:56 - INFO - __main__ -   Saving model checkpoint to ./Bert_Classifier/\n","11/14/2019 09:04:56 - INFO - transformers.configuration_utils -   Configuration saved in ./Bert_Classifier/config.json\n","11/14/2019 09:04:57 - INFO - transformers.modeling_utils -   Model weights saved in ./Bert_Classifier/pytorch_model.bin\n","11/14/2019 09:04:57 - INFO - transformers.configuration_utils -   loading configuration file ./Bert_Classifier/config.json\n","11/14/2019 09:04:57 - INFO - transformers.configuration_utils -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"cola\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n","11/14/2019 09:04:57 - INFO - transformers.modeling_utils -   loading weights file ./Bert_Classifier/pytorch_model.bin\n","11/14/2019 09:05:00 - INFO - transformers.tokenization_utils -   Model name './Bert_Classifier/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './Bert_Classifier/' is a path or url to a directory containing tokenizer files.\n","11/14/2019 09:05:00 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/vocab.txt\n","11/14/2019 09:05:00 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/added_tokens.json\n","11/14/2019 09:05:00 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/special_tokens_map.json\n","11/14/2019 09:05:00 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/tokenizer_config.json\n","11/14/2019 09:05:00 - INFO - transformers.tokenization_utils -   Model name './Bert_Classifier/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './Bert_Classifier/' is a path or url to a directory containing tokenizer files.\n","11/14/2019 09:05:00 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/vocab.txt\n","11/14/2019 09:05:00 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/added_tokens.json\n","11/14/2019 09:05:00 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/special_tokens_map.json\n","11/14/2019 09:05:00 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/tokenizer_config.json\n","11/14/2019 09:05:00 - INFO - __main__ -   Evaluate the following checkpoints: ['./Bert_Classifier/']\n","11/14/2019 09:05:00 - INFO - transformers.configuration_utils -   loading configuration file ./Bert_Classifier/config.json\n","11/14/2019 09:05:00 - INFO - transformers.configuration_utils -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"cola\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n","11/14/2019 09:05:00 - INFO - transformers.modeling_utils -   loading weights file ./Bert_Classifier/pytorch_model.bin\n","11/14/2019 09:05:04 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/09_BERT/Small_Dataset/cached_dev_bert-base-uncased_128_cola\n","11/14/2019 09:05:04 - INFO - __main__ -   ***** Running evaluation  *****\n","11/14/2019 09:05:04 - INFO - __main__ -     Num examples = 100\n","11/14/2019 09:05:04 - INFO - __main__ -     Batch size = 8\n","Evaluating: 100% 13/13 [00:00<00:00, 26.50it/s]\n","11/14/2019 09:05:04 - INFO - __main__ -   ***** Eval results  *****\n","11/14/2019 09:05:04 - INFO - __main__ -     mcc = 0.24077170617153837\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u1SZ9fLzHrPM","colab_type":"text"},"source":["If you trained with the small dataset you should see a final result such as mcc = 0.24.\n","\n","On the other hand if you trained with the large one mcc increases to 0.70\n","\n","The MCC score measures how well does the algorithm perform on both positive and negative predictions.\n","\n","It gives more information than the accuracy or the f1 score.\n","\n","This numbers ranges from -1 to 1 being 0 the random case, -1 the worst value and +1 the best value."]},{"cell_type":"markdown","metadata":{"id":"sUmytTFLWSZp","colab_type":"text"},"source":["## Further Evaluation\n","\n","Let's compute the metrics of our fine-tuned model to see how well it performs on the test set"]},{"cell_type":"code","metadata":{"id":"Au85e90b2Ed6","colab_type":"code","outputId":"8a97a653-0f58-40ae-ef8f-7513bdf2c3f6","executionInfo":{"status":"ok","timestamp":1573727482856,"user_tz":-60,"elapsed":773,"user":{"displayName":"Expert System - Lab","photoUrl":"","userId":"16197418968100245121"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["import numpy as np\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import matthews_corrcoef\n","\n","preds = np.loadtxt(\"./Bert_Classifier/predictions.txt\")\n","test = pd.read_csv(os.environ[\"DATA_DIR\"] + \"dev.tsv\", header=None, sep = '\\t')\n","\n","print(classification_report(np.asarray(test[1]), preds))\n","print(\"Accuracy: \", accuracy_score(np.asarray(test[1]), preds))\n","print(\"MCC: \", matthews_corrcoef(test[1], preds))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.66      0.61      0.63        54\n","           1       0.58      0.63      0.60        46\n","\n","    accuracy                           0.62       100\n","   macro avg       0.62      0.62      0.62       100\n","weighted avg       0.62      0.62      0.62       100\n","\n","Accuracy:  0.62\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M1l41weoHzUW","colab_type":"text"},"source":["You should see an accuracy of 62% and an f1-score of 62% which is a good result considering the size of the dataset."]},{"cell_type":"markdown","metadata":{"id":"pSGsGLSNFkpr","colab_type":"text"},"source":["The full model fine-tuned on the 500k tweets achieve the following metrics:\n","\n","    - Accuracy = 0.85\n","    - Recall = 0.85\n","    - Precision = 0.86\n","    - Recall = 0.85\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_yIjsNITWfGg","colab_type":"text"},"source":["## Perform inference\n","\n","Now let's take some random examples from our test set:"]},{"cell_type":"code","metadata":{"id":"4zyHDRIKBSeN","colab_type":"code","outputId":"1ad7ad38-d9af-4304-85fd-341c19f808c6","executionInfo":{"status":"ok","timestamp":1573720434782,"user_tz":-60,"elapsed":902,"user":{"displayName":"Expert System - Lab","photoUrl":"","userId":"16197418968100245121"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["os.mkdir(\"./Test_Dataset/\") # We are going to store the test dataset in this folder\n","\n","test_evaluate = test[:4]\n","print(test_evaluate)\n","test_evaluate.to_csv(\"./Test_Dataset/dev.tsv\", sep='\\t', index=False, header=False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   0  1  2                                                  3\n","0  0  1  a  Now Playing: ♬ Dick Curless - Evil Hearted Me ...\n","1  1  0  a  Not only are you comfortably swaddled in secur...\n","2  2  1  a  Follow @iAmMySign !!!  Follow @iAmMySign our o...\n","3  3  0  a  These strawberry sandwich cookies are so easy ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ry28Kp1oIZgR","colab_type":"text"},"source":["If you want to perform inference with the larger model we provide an already trained version. You only have to change the argument in model_name_or path from Bert_Classifier_small to Bert_Classifier_Large"]},{"cell_type":"code","metadata":{"id":"sQHEysl0QQWq","colab_type":"code","outputId":"f47ca46c-3f0d-44cb-bbd7-6c9abe288994","executionInfo":{"status":"ok","timestamp":1573732158809,"user_tz":-60,"elapsed":1029,"user":{"displayName":"Expert System - Lab","photoUrl":"","userId":"16197418968100245121"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%env MODEL_PATH=./Bert_Classifier/\n","\n","#Uncomment the following line to use the version of the model trained with the large dataset\n","#%env MODEL_PATH=/content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/"],"execution_count":16,"outputs":[{"output_type":"stream","text":["env: MODEL_PATH=./Bert_Classifier/\n","env: MODEL_PATH=/content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bWOTIq74DqOj","colab_type":"code","outputId":"db23b4fc-4ac3-458c-8ae7-ea4e027127ee","executionInfo":{"status":"ok","timestamp":1573732302563,"user_tz":-60,"elapsed":16796,"user":{"displayName":"Expert System - Lab","photoUrl":"","userId":"16197418968100245121"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python /content/gdrive/My\\ Drive/09_BERT/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path \"$MODEL_PATH\" \\\n","    --task_name CoLA \\\n","    --do_eval \\\n","    --do_lower_case \\\n","    --data_dir ./Test_Dataset/ \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=8   \\\n","    --per_gpu_train_batch_size=8   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1.0 \\\n","    --save_steps 62500 \\\n","    --output_dir  \"$MODEL_PATH\""],"execution_count":19,"outputs":[{"output_type":"stream","text":["11/14/2019 11:51:29 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","11/14/2019 11:51:29 - INFO - transformers.configuration_utils -   loading configuration file /content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/config.json\n","11/14/2019 11:51:29 - INFO - transformers.configuration_utils -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"cola\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n","11/14/2019 11:51:29 - INFO - transformers.tokenization_utils -   Model name '/content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/' is a path or url to a directory containing tokenizer files.\n","11/14/2019 11:51:29 - INFO - transformers.tokenization_utils -   loading file /content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/vocab.txt\n","11/14/2019 11:51:29 - INFO - transformers.tokenization_utils -   loading file /content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/added_tokens.json\n","11/14/2019 11:51:29 - INFO - transformers.tokenization_utils -   loading file /content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/special_tokens_map.json\n","11/14/2019 11:51:29 - INFO - transformers.tokenization_utils -   loading file /content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/tokenizer_config.json\n","11/14/2019 11:51:29 - INFO - transformers.modeling_utils -   loading weights file /content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/pytorch_model.bin\n","11/14/2019 11:51:36 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='./Test_Dataset/', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='/content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=62500, seed=42, server_ip='', server_port='', task_name='cola', tokenizer_name='', tpu=False, tpu_ip_address='', tpu_name='', warmup_steps=0, weight_decay=0.0, xrt_tpu_config='')\n","11/14/2019 11:51:36 - INFO - transformers.tokenization_utils -   Model name '/content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/' is a path or url to a directory containing tokenizer files.\n","11/14/2019 11:51:36 - INFO - transformers.tokenization_utils -   loading file /content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/vocab.txt\n","11/14/2019 11:51:36 - INFO - transformers.tokenization_utils -   loading file /content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/added_tokens.json\n","11/14/2019 11:51:36 - INFO - transformers.tokenization_utils -   loading file /content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/special_tokens_map.json\n","11/14/2019 11:51:36 - INFO - transformers.tokenization_utils -   loading file /content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/tokenizer_config.json\n","11/14/2019 11:51:37 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/']\n","11/14/2019 11:51:37 - INFO - transformers.configuration_utils -   loading configuration file /content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/config.json\n","11/14/2019 11:51:37 - INFO - transformers.configuration_utils -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"cola\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n","11/14/2019 11:51:37 - INFO - transformers.modeling_utils -   loading weights file /content/gdrive/My Drive/09_BERT/Bert_Classifier_Large/pytorch_model.bin\n","11/14/2019 11:51:40 - INFO - __main__ -   Loading features from cached file ./Test_Dataset/cached_dev_Bert_Classifier_Large_128_cola\n","11/14/2019 11:51:40 - INFO - __main__ -   ***** Running evaluation  *****\n","11/14/2019 11:51:40 - INFO - __main__ -     Num examples = 4\n","11/14/2019 11:51:40 - INFO - __main__ -     Batch size = 8\n","Evaluating: 100% 1/1 [00:00<00:00, 39.15it/s]\n","11/14/2019 11:51:40 - INFO - __main__ -   ***** Eval results  *****\n","11/14/2019 11:51:40 - INFO - __main__ -     mcc = 1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XC6oati6H-cx","colab_type":"text"},"source":["mcc\n","\n","Let's see if the model has correctly classified the examples:"]},{"cell_type":"code","metadata":{"id":"7chkHgKMECc1","colab_type":"code","outputId":"d1b366e3-93fd-414d-ce98-8a5759c4713a","executionInfo":{"status":"ok","timestamp":1573720501271,"user_tz":-60,"elapsed":910,"user":{"displayName":"Expert System - Lab","photoUrl":"","userId":"16197418968100245121"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import os\n","\n","results = np.loadtxt(os.environ['MODEL_PATH'] + \"predictions.txt\")\n","for i,t in enumerate(test_evaluate[3]):\n","    print(t + \" --> \", \"BOT\" if results[i]> 0.5 else \"NOT A BOT\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Now Playing: ♬ Dick Curless - Evil Hearted Me ♬ https://t.co/fzgP9IRt2h -->  BOT\n","Not only are you comfortably swaddled in security today, it’s ... More for Capricorn https://t.co/MVCHEli4g1 -->  NOT A BOT\n","Follow @iAmMySign !!!  Follow @iAmMySign our official page for the whole Zodiac.  Follow @iAmMySign !!!  Follow @iAmMySign !!! -->  BOT\n","These strawberry sandwich cookies are so easy to make and so tasty! Perfect for #MothersDay https://t.co/Uq7cooR2y7 via @iamthemaven -->  NOT A BOT\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eUX8iM5tFZvn","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"gE6tv1TEzYsr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}