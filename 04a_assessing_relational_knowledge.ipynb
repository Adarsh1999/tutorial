{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04a_assessing_relational_knowledge",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vkl7mxsBpn6E",
        "colab_type": "text"
      },
      "source": [
        "# Assessing Relational Knowledge captured by embeddings\n",
        "\n",
        "In this notebook, we use the `embrela` library to study  whether various embedding spaces capture certain lexico-semantic relations on WordNet. The approach behind `embrela` is described in:\n",
        "\n",
        "Denaux, R. and Gomez-Perez, J.M., 2019, September.\n",
        "*Assessing the Lexico-Semantic Relational Knowledge Captured by Word and Concept Embeddings.* In Proceedings of the 10th International Conference on Knowledge Capture (pp. 29-36). ACM. [preprint](https://arxiv.org/abs/1909.11042)\n",
        "\n",
        "The overall pipeline looks as follows:\n",
        "\n",
        "![embrela pipeline](https://github.com/rdenaux/embrelassess/raw/master/wnet-rel-pair-extractor/src/embrel-assess-pipeline.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKhhkYugrdnm",
        "colab_type": "text"
      },
      "source": [
        "### Download `embrela` project\n",
        "We put the main python module on the main working folder to make importing of submodules a bit easier to read.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bmXQvaowNYm",
        "colab_type": "code",
        "outputId": "359d3dae-3c66-4406-f064-a9fbc158f6a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "!git clone https://github.com/rdenaux/embrelassess.git embrela_prj\n",
        "!ln -s embrela_prj/embrelassess embrelassess"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'embrela_prj'...\n",
            "remote: Enumerating objects: 304, done.\u001b[K\n",
            "remote: Counting objects: 100% (304/304), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 304 (delta 138), reused 295 (delta 132), pack-reused 0\n",
            "Receiving objects: 100% (304/304), 14.64 MiB | 15.75 MiB/s, done.\n",
            "Resolving deltas: 100% (138/138), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHkfMnDlrRFz",
        "colab_type": "text"
      },
      "source": [
        "## 1. Download generated datasets\n",
        "Instead of generating datasets from scratch, which can be done by using the `embrela_prj/wnet-rel-pair-extractor`, in this notebook we'll use a set of pre-generated datasets extracted from WordNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwAHbFOI3JQN",
        "colab_type": "code",
        "outputId": "936fd9de-be81-42d8-c331-ce6c2c96e8fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "!wget https://github.com/rdenaux/embrelassess/releases/download/v0.1/vocabrels_wnet-switched-negs.zip\n",
        "!unzip vocabrels_wnet-switched-negs.zip"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-19 01:23:30--  https://github.com/rdenaux/embrelassess/releases/download/v0.1/vocabrels_wnet-switched-negs.zip\n",
            "Resolving github.com (github.com)... 140.82.118.4\n",
            "Connecting to github.com (github.com)|140.82.118.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/210606207/573bc500-092b-11ea-854d-e8b23654ea2e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191119%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191119T012330Z&X-Amz-Expires=300&X-Amz-Signature=e6368a0407decf52da13e0dd2eba2ba1357d2f8a56d2be8ba477515b6879c374&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dvocabrels_wnet-switched-negs.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-11-19 01:23:30--  https://github-production-release-asset-2e65be.s3.amazonaws.com/210606207/573bc500-092b-11ea-854d-e8b23654ea2e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191119%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191119T012330Z&X-Amz-Expires=300&X-Amz-Signature=e6368a0407decf52da13e0dd2eba2ba1357d2f8a56d2be8ba477515b6879c374&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dvocabrels_wnet-switched-negs.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.9.4\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8844325 (8.4M) [application/octet-stream]\n",
            "Saving to: ‘vocabrels_wnet-switched-negs.zip.1’\n",
            "\n",
            "vocabrels_wnet-swit 100%[===================>]   8.43M  11.0MB/s    in 0.8s    \n",
            "\n",
            "2019-11-19 01:23:31 (11.0 MB/s) - ‘vocabrels_wnet-switched-negs.zip.1’ saved [8844325/8844325]\n",
            "\n",
            "Archive:  vocabrels_wnet-switched-negs.zip\n",
            "replace vocabrels_wn-switched-negs/lem2lem_also_see__5800.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace vocabrels_wn-switched-negs/lem2lem_antonym__9310.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiq9p3iitAje",
        "colab_type": "text"
      },
      "source": [
        "We can load the metadata of the generated relations as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFgsWWbr4D5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import embrelassess.learn as learn\n",
        "import os.path as osp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcQQ5PBnokZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rel_path = osp.join('vocabrels_wn-switched-negs/')\n",
        "rels_df = learn.load_rels_meta(rel_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOZaWPGWtLZm",
        "colab_type": "text"
      },
      "source": [
        "Which gives us a pandas DataFrame with metadata about the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqPvZcvxpstc",
        "colab_type": "code",
        "outputId": "2baa9e0a-7ee8-44bf-a1c6-82a98e536d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        }
      },
      "source": [
        "rels_df"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>name</th>\n",
              "      <th>cnt</th>\n",
              "      <th>file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>member_of_category_domain</td>\n",
              "      <td>9116</td>\n",
              "      <td>lem2lem_member_of_category_domain__9116.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>synonym</td>\n",
              "      <td>74822</td>\n",
              "      <td>lem2lem_synonym__74822.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>entailment</td>\n",
              "      <td>1519</td>\n",
              "      <td>lem2lem_entailment__1519.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>part_meronym</td>\n",
              "      <td>6403</td>\n",
              "      <td>lem2lem_part_meronym__6403.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>region_domain</td>\n",
              "      <td>1349</td>\n",
              "      <td>lem2lem_region_domain__1349.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>instances_hyponym</td>\n",
              "      <td>2358</td>\n",
              "      <td>lem2lem_instances_hyponym__2358.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>member_meronym</td>\n",
              "      <td>1315</td>\n",
              "      <td>lem2lem_member_meronym__1315.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>instance_hypernym</td>\n",
              "      <td>2358</td>\n",
              "      <td>lem2lem_instance_hypernym__2358.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>member_of_region_domain</td>\n",
              "      <td>1349</td>\n",
              "      <td>lem2lem_member_of_region_domain__1349.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>hypernym</td>\n",
              "      <td>110650</td>\n",
              "      <td>lem2lem_hypernym__110650.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>substance_holonym</td>\n",
              "      <td>369</td>\n",
              "      <td>lem2lem_substance_holonym__369.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>hyponym</td>\n",
              "      <td>110650</td>\n",
              "      <td>lem2lem_hyponym__110650.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>cause</td>\n",
              "      <td>719</td>\n",
              "      <td>lem2lem_cause__719.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>similar</td>\n",
              "      <td>20464</td>\n",
              "      <td>lem2lem_similar__20464.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>attribute</td>\n",
              "      <td>1718</td>\n",
              "      <td>lem2lem_attribute__1718.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>usage_domain</td>\n",
              "      <td>846</td>\n",
              "      <td>lem2lem_usage_domain__846.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>pertainym</td>\n",
              "      <td>6516</td>\n",
              "      <td>lem2lem_pertainym__6516.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>substance_meronym</td>\n",
              "      <td>369</td>\n",
              "      <td>lem2lem_substance_meronym__369.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>verb_group</td>\n",
              "      <td>4944</td>\n",
              "      <td>lem2lem_verb_group__4944.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>category_domain</td>\n",
              "      <td>9116</td>\n",
              "      <td>lem2lem_category_domain__9116.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>derivation</td>\n",
              "      <td>118888</td>\n",
              "      <td>lem2lem_derivation__118888.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>antonym</td>\n",
              "      <td>9310</td>\n",
              "      <td>lem2lem_antonym__9310.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>member_of_usage_domain</td>\n",
              "      <td>846</td>\n",
              "      <td>lem2lem_member_of_usage_domain__846.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>member_holonym</td>\n",
              "      <td>1315</td>\n",
              "      <td>lem2lem_member_holonym__1315.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>also_see</td>\n",
              "      <td>5800</td>\n",
              "      <td>lem2lem_also_see__5800.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>part_holonym</td>\n",
              "      <td>6403</td>\n",
              "      <td>lem2lem_part_holonym__6403.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>participle_of</td>\n",
              "      <td>81</td>\n",
              "      <td>lem2lem_participle_of__81.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       type  ...                                         file\n",
              "0   lem2lem  ...  lem2lem_member_of_category_domain__9116.txt\n",
              "1   lem2lem  ...                   lem2lem_synonym__74822.txt\n",
              "2   lem2lem  ...                 lem2lem_entailment__1519.txt\n",
              "3   lem2lem  ...               lem2lem_part_meronym__6403.txt\n",
              "4   lem2lem  ...              lem2lem_region_domain__1349.txt\n",
              "5   lem2lem  ...          lem2lem_instances_hyponym__2358.txt\n",
              "6   lem2lem  ...             lem2lem_member_meronym__1315.txt\n",
              "7   lem2lem  ...          lem2lem_instance_hypernym__2358.txt\n",
              "8   lem2lem  ...    lem2lem_member_of_region_domain__1349.txt\n",
              "9   lem2lem  ...                 lem2lem_hypernym__110650.txt\n",
              "10  lem2lem  ...           lem2lem_substance_holonym__369.txt\n",
              "11  lem2lem  ...                  lem2lem_hyponym__110650.txt\n",
              "12  lem2lem  ...                       lem2lem_cause__719.txt\n",
              "13  lem2lem  ...                   lem2lem_similar__20464.txt\n",
              "14  lem2lem  ...                  lem2lem_attribute__1718.txt\n",
              "15  lem2lem  ...                lem2lem_usage_domain__846.txt\n",
              "16  lem2lem  ...                  lem2lem_pertainym__6516.txt\n",
              "17  lem2lem  ...           lem2lem_substance_meronym__369.txt\n",
              "18  lem2lem  ...                 lem2lem_verb_group__4944.txt\n",
              "19  lem2lem  ...            lem2lem_category_domain__9116.txt\n",
              "20  lem2lem  ...               lem2lem_derivation__118888.txt\n",
              "21  lem2lem  ...                    lem2lem_antonym__9310.txt\n",
              "22  lem2lem  ...      lem2lem_member_of_usage_domain__846.txt\n",
              "23  lem2lem  ...             lem2lem_member_holonym__1315.txt\n",
              "24  lem2lem  ...                   lem2lem_also_see__5800.txt\n",
              "25  lem2lem  ...               lem2lem_part_holonym__6403.txt\n",
              "26  lem2lem  ...                lem2lem_participle_of__81.txt\n",
              "\n",
              "[27 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcBNuJw-tkJ1",
        "colab_type": "text"
      },
      "source": [
        "The previous step should have printed a table with 27 rows. One for each generated dataset. All of the datasets are `lem2lem`, i.e. the *source* and *target*s are both lemmas. In the paper we also consider pairs of types `lem2syn`, `syn2syn` and `lem2pos`, where `syn` is a synset (or syncon) and `pos` is part-of-speech. \n",
        "\n",
        "The remaining columsn in the table tell us:\n",
        "* the `name` of the relation \n",
        "* `cnt`: the number of **positive** examples extracted from the KG, WorNet 3 in this case.\n",
        "* the `file` name where we can find both the positive and negative examples\n",
        "\n",
        "Note that each dataset will have about twice the number of lines as the positive `cnt`, since we aim to build balanced datasets. For example for the `entailment` relation, we have 1519 positive pairs, but:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVQocJqouX7Y",
        "colab_type": "code",
        "outputId": "3115c23b-520f-448b-bb5d-0b5e48d20bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!wc -l vocabrels_wn-switched-negs/lem2lem_entailment__1519.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3039 vocabrels_wn-switched-negs/lem2lem_entailment__1519.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_nBGsqSvTy8",
        "colab_type": "text"
      },
      "source": [
        "3039 lines in total. Further inspection of the file shows that it's a tab-separated-value with columns:\n",
        "`source`, `target`, `label` and `comment`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMc4BvCvwAR0",
        "colab_type": "code",
        "outputId": "aaa8cf41-7d6c-41ad-e541-9110d38c1c14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!head -n 5 vocabrels_wn-switched-negs/lem2lem_antonym__9310.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "specialize\tdiversify\t1\tpositive\n",
            "give off\taffirm\t0\t[NegSwitched]\n",
            "beginning\tending\t1\tpositive\n",
            "take off\tdc\t0\t[NegSwitched]\n",
            "hot\tcold\t1\tpositive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRjX-vfbNsBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "e4ddc574-b1be-4b03-fd5f-febc30c47008"
      },
      "source": [
        "!head -n 5 HolE_wn_rel_datasets/syn2syn_antonym__7295.txt"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wn31_ADJECTIVE#1413922\twn31_ADJECTIVE#1414148\t1\tpositive\n",
            "wn31_ADJECTIVE#463344\twn31_ADJECTIVE#511660\t0\t[NegSwitched]\n",
            "wn31_ADJECTIVE#1151786\twn31_ADJECTIVE#1152997\t1\tpositive\n",
            "wn31_NOUN#4587272\twn31_ADVERB#328642\t0\t[NegSwitched]\n",
            "wn31_ADJECTIVE#1676037\twn31_ADJECTIVE#1676186\t1\tpositive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l53Bw-KLqboP",
        "colab_type": "text"
      },
      "source": [
        "## 2. Load the embeddings to be evaluated\n",
        "\n",
        "We will use the following embeddings:\n",
        "* wordnet embeddings trained using **HolE**, which learns embeddings directly from the WordNet graph. This is a strong baseline\n",
        "* **FastText**\n",
        "* **GloVe**\n",
        "\n",
        "First, we download the pre-trained HolE embeddings. These have been trained for 500 epochs and have 150 dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b59_Y7cG3b9l",
        "colab_type": "code",
        "outputId": "340a279f-af3f-4b54-e302-c65a05957c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "source": [
        "!wget https://github.com/rdenaux/embrelassess/releases/download/v0.1.1/wn-en-3.1.-HolE-500e-150d.vec.tar.gz\n",
        "!tar xzf wn-en-3.1.-HolE-500e-150d.vec.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-18 14:59:39--  https://github.com/rdenaux/embrelassess/releases/download/v0.1.1/wn-en-3.1.-HolE-500e-150d.vec.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.118.3\n",
            "Connecting to github.com (github.com)|140.82.118.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/210606207/46915a80-09d0-11ea-91aa-ac4fb6373635?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191118%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191118T145940Z&X-Amz-Expires=300&X-Amz-Signature=2ce7687d2dfa7ed6bced1d0e0d73b754cdae88ddbd5047a38e6a9a5a3a5be585&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dwn-en-3.1.-HolE-500e-150d.vec.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-11-18 14:59:40--  https://github-production-release-asset-2e65be.s3.amazonaws.com/210606207/46915a80-09d0-11ea-91aa-ac4fb6373635?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191118%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191118T145940Z&X-Amz-Expires=300&X-Amz-Signature=2ce7687d2dfa7ed6bced1d0e0d73b754cdae88ddbd5047a38e6a9a5a3a5be585&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dwn-en-3.1.-HolE-500e-150d.vec.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.169.243\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.169.243|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 369436200 (352M) [application/octet-stream]\n",
            "Saving to: ‘wn-en-3.1.-HolE-500e-150d.vec.tar.gz’\n",
            "\n",
            "wn-en-3.1.-HolE-500 100%[===================>] 352.32M  36.0MB/s    in 10s     \n",
            "\n",
            "2019-11-18 14:59:51 (33.8 MB/s) - ‘wn-en-3.1.-HolE-500e-150d.vec.tar.gz’ saved [369436200/369436200]\n",
            "\n",
            "tar (child): wn-en-3.0-HolE-500e-150d.vec.tar.gz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cDxGIwlxlbc",
        "colab_type": "text"
      },
      "source": [
        "Next, we can load the embeddings. The `embrela` library, as well as `torchtext` has various convenience methods to do this. `torchtext` automatically downloads pre-trained embeddings published at the official FastText and Glove sites."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Ni566hx7ZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import embrelassess.embloader as embloader\n",
        "#import torchtext\n",
        "from torchtext.vocab import Vectors, FastText, GloVe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz0VtxF9yQw9",
        "colab_type": "text"
      },
      "source": [
        "`torchtext` [defines a number of `aliases`](https://torchtext.readthedocs.io/en/latest/vocab.html#pretrained-aliases) for pre-trained embeddings. We use:\n",
        "* the `glove.6B.100d` pre-trained embeddings. In the paper we've used the `glove.840B.300d`, but these take longer to download\n",
        "* the `fasttext.simple.300d` embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeehWpy7zRWp",
        "colab_type": "code",
        "outputId": "f95a6fd8-f8bd-4355-a9b7-5041d12a700b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# uncomment next line if you want to use GloVe embeddings. These can take a while to load\n",
        "# glove_en = GloVe(name='6B', dim=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                           \n",
            "100%|█████████▉| 399723/400000 [00:15<00:00, 25505.80it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkGCCl9z5ObL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uncomment if you want to use FastText. These can take a while to load\n",
        "FastText.url_base = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.{}.vec'\n",
        "#ft_en = FastText(language='en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfLq8OVtyBde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "holE_wnet_en = embloader.TSVVectors('wn-en-3.1-HolE-500e.vec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qBKYXWKCtk7",
        "colab_type": "code",
        "outputId": "163c0c6b-ea8f-4e97-bf2d-1bfc0863b014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(holE_wnet_en)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "embrelassess.embloader.TSVVectors"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO8vjDYNwnAa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4922256-1b74-4bbf-9451-1cd222d7c84b"
      },
      "source": [
        "len(list(holE_wnet_en.stoi.keys()))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "264965"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLk6iRT7wzF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('wn-en-3.1-HolE-500e.vocab.txt', 'w') as fout:\n",
        "  for syn in list(holE_wnet_en.stoi.keys()):\n",
        "    fout.write(syn + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KksjZyYhJhUU",
        "colab_type": "text"
      },
      "source": [
        "We will also use random vectors as another baseline and to filter out biased relations. We use the vocabulary of words and syncons used in the K-CAP'19 paper, which was derived from disambiguating the UN corpus using Cogito."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvH0yMzfEsSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_path = osp.join('embrela_prj/vocab_sensi_lemsyn_UN.txt')\n",
        "rnd_vecs = embloader.RandomVectors(vocab_path, dim=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnrv8-uUJ5ep",
        "colab_type": "text"
      },
      "source": [
        "The training phase expects to get a map of vector space ids to `VecPairLoader` instances, which will take care of mapping `source` and `target` words in the generated datasets into the appropriate embeddings. Here we define the data loaders to use. Uncomment others if you want to use other embedding spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvtvsACpCvk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_loaders = {\n",
        "        #'glove_cc_en':    embloader.VecPairLoader(glove_en),\n",
        "        #'ft_wikip_en':    embloader.VecPairLoader(ft_en),\n",
        "        #'vecsi_wiki_en': embloader.VecPairLoader(vecsi_wiki_en),\n",
        "        #'vecsi_un_en':   embloader.VecPairLoader(vecsi_un_en),\n",
        "        'rand_en':        embloader.VecPairLoader(rnd_vecs),\n",
        "        'holE_wnet_en':   embloader.VecPairLoader(holE_wnet_en)\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8gKLRpjKW3A",
        "colab_type": "text"
      },
      "source": [
        "## Learn models\n",
        "Now that we have the datasets and the embeddings, we are ready to train some models. This step is highly configurable, but in this notebook we'll:\n",
        "* only train models with the `nn3` architecture (ie with 3 fully connected layers)\n",
        "* only train models for a couple of (the 27) relations to keep execution short\n",
        "* only train 3 models per embedding/relation/architecture combination\n",
        "* apply *input perturbation* as explained in the paper which shifts both `source` and `target` embeddings by the same amount\n",
        "\n",
        "The trained models and evaluation results will be written to an output folder. Even with this restricted setup, this step can take 5 to 10 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka7xvBENLD_S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4c3c33c-49aa-4bb1-ff74-ef6eba21d7d8"
      },
      "source": [
        "model_archs = ['nn3']\n",
        "n_runs = 3\n",
        "my_rels = ['entailment', 'antonym']\n",
        "def only_with_names(relname_whitelist):\n",
        "  return lambda df_row: df_row['name'] in relname_whitelist\n",
        "\n",
        "odir = 'experiment/trained_models/'\n",
        "\n",
        "learn_results = learn.learn_rels(\n",
        "    rel_path, rels_df, data_loaders,\n",
        "    models=model_archs, n_runs=n_runs,\n",
        "    rel_filter=only_with_names(my_rels),\n",
        "    train_input_disturber_for_vec=learn.pair_disturber_for_vectors,\n",
        "    odir_path=odir, cuda=True)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 4.77 µs\n",
            "\n",
            "*** rel 0 of 27 ***\n",
            "\n",
            "region_domain lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 1 of 27 ***\n",
            "\n",
            "cause lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 2 of 27 ***\n",
            "\n",
            "Training each model 3 times...\n",
            "run 0 on model nn3 with vectors rand_en\n",
            "\n",
            " lem2lem_antonym__9310.txt\n",
            "train (16758,), validate (931,), test (932,)\n",
            "fcs 4, dropouts 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NNBiClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished 12 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 932, 'threshold': 1.0, 'examples_above_threshold': 932, 'correct': 452, 'tp': 218, 'fp': 240, 'fn': 240}\n",
            "For randpredict on 932(100.00%) of 932 examples > 1.00 confidence, prec, rec, acc, f1: 47 47 48 47 %\n",
            "run 1 on model nn3 with vectors rand_en\n",
            "\n",
            " lem2lem_antonym__9310.txt\n",
            "train (16758,), validate (931,), test (932,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 12 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 932, 'threshold': 1.0, 'examples_above_threshold': 932, 'correct': 426, 'tp': 205, 'fp': 253, 'fn': 253}\n",
            "For randpredict on 932(100.00%) of 932 examples > 1.00 confidence, prec, rec, acc, f1: 44 44 45 44 %\n",
            "run 2 on model nn3 with vectors rand_en\n",
            "\n",
            " lem2lem_antonym__9310.txt\n",
            "train (16758,), validate (931,), test (932,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 12 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 932, 'threshold': 1.0, 'examples_above_threshold': 932, 'correct': 470, 'tp': 227, 'fp': 231, 'fn': 231}\n",
            "For randpredict on 932(100.00%) of 932 examples > 1.00 confidence, prec, rec, acc, f1: 49 49 50 49 %\n",
            "run 0 on model nn3 with vectors holE_wnet_en\n",
            "\n",
            " lem2lem_antonym__9310.txt\n",
            "train (16758,), validate (931,), test (932,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 12 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 932, 'threshold': 1.0, 'examples_above_threshold': 932, 'correct': 478, 'tp': 231, 'fp': 227, 'fn': 227}\n",
            "For randpredict on 932(100.00%) of 932 examples > 1.00 confidence, prec, rec, acc, f1: 50 50 51 50 %\n",
            "run 1 on model nn3 with vectors holE_wnet_en\n",
            "\n",
            " lem2lem_antonym__9310.txt\n",
            "train (16758,), validate (931,), test (932,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 12 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 932, 'threshold': 1.0, 'examples_above_threshold': 932, 'correct': 464, 'tp': 224, 'fp': 234, 'fn': 234}\n",
            "For randpredict on 932(100.00%) of 932 examples > 1.00 confidence, prec, rec, acc, f1: 48 48 49 48 %\n",
            "run 2 on model nn3 with vectors holE_wnet_en\n",
            "\n",
            " lem2lem_antonym__9310.txt\n",
            "train (16758,), validate (931,), test (932,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 12 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 932, 'threshold': 1.0, 'examples_above_threshold': 932, 'correct': 460, 'tp': 222, 'fp': 236, 'fn': 236}\n",
            "For randpredict on 932(100.00%) of 932 examples > 1.00 confidence, prec, rec, acc, f1: 48 48 49 48 %\n",
            "\n",
            "*** rel 3 of 27 ***\n",
            "\n",
            "similar lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 4 of 27 ***\n",
            "\n",
            "verb_group lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 5 of 27 ***\n",
            "\n",
            "substance_holonym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 6 of 27 ***\n",
            "\n",
            "derivation lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 7 of 27 ***\n",
            "\n",
            "synonym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 8 of 27 ***\n",
            "\n",
            "pertainym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 9 of 27 ***\n",
            "\n",
            "participle_of lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 10 of 27 ***\n",
            "\n",
            "attribute lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 11 of 27 ***\n",
            "\n",
            "part_meronym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 12 of 27 ***\n",
            "\n",
            "hypernym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 13 of 27 ***\n",
            "\n",
            "member_holonym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 14 of 27 ***\n",
            "\n",
            "Training each model 3 times...\n",
            "run 0 on model nn3 with vectors rand_en\n",
            "\n",
            " lem2lem_entailment__1519.txt\n",
            "train (2735,), validate (151,), test (153,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 24 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 153, 'threshold': 1.0, 'examples_above_threshold': 153, 'correct': 69, 'tp': 32, 'fp': 40, 'fn': 44}\n",
            "For randpredict on 153(100.00%) of 153 examples > 1.00 confidence, prec, rec, acc, f1: 44 42 45 43 %\n",
            "run 1 on model nn3 with vectors rand_en\n",
            "\n",
            " lem2lem_entailment__1519.txt\n",
            "train (2735,), validate (151,), test (153,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 24 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 153, 'threshold': 1.0, 'examples_above_threshold': 153, 'correct': 81, 'tp': 38, 'fp': 34, 'fn': 38}\n",
            "For randpredict on 153(100.00%) of 153 examples > 1.00 confidence, prec, rec, acc, f1: 52 50 52 51 %\n",
            "run 2 on model nn3 with vectors rand_en\n",
            "\n",
            " lem2lem_entailment__1519.txt\n",
            "train (2735,), validate (151,), test (153,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 24 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 153, 'threshold': 1.0, 'examples_above_threshold': 153, 'correct': 71, 'tp': 33, 'fp': 39, 'fn': 43}\n",
            "For randpredict on 153(100.00%) of 153 examples > 1.00 confidence, prec, rec, acc, f1: 45 43 46 44 %\n",
            "run 0 on model nn3 with vectors holE_wnet_en\n",
            "\n",
            " lem2lem_entailment__1519.txt\n",
            "train (2735,), validate (151,), test (153,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 24 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 153, 'threshold': 1.0, 'examples_above_threshold': 153, 'correct': 77, 'tp': 36, 'fp': 36, 'fn': 40}\n",
            "For randpredict on 153(100.00%) of 153 examples > 1.00 confidence, prec, rec, acc, f1: 50 47 50 48 %\n",
            "run 1 on model nn3 with vectors holE_wnet_en\n",
            "\n",
            " lem2lem_entailment__1519.txt\n",
            "train (2735,), validate (151,), test (153,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 24 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 153, 'threshold': 1.0, 'examples_above_threshold': 153, 'correct': 81, 'tp': 38, 'fp': 34, 'fn': 38}\n",
            "For randpredict on 153(100.00%) of 153 examples > 1.00 confidence, prec, rec, acc, f1: 52 50 52 51 %\n",
            "run 2 on model nn3 with vectors holE_wnet_en\n",
            "\n",
            " lem2lem_entailment__1519.txt\n",
            "train (2735,), validate (151,), test (153,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 24 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 153, 'threshold': 1.0, 'examples_above_threshold': 153, 'correct': 79, 'tp': 37, 'fp': 35, 'fn': 39}\n",
            "For randpredict on 153(100.00%) of 153 examples > 1.00 confidence, prec, rec, acc, f1: 51 48 51 50 %\n",
            "\n",
            "*** rel 15 of 27 ***\n",
            "\n",
            "hyponym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 16 of 27 ***\n",
            "\n",
            "category_domain lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 17 of 27 ***\n",
            "\n",
            "substance_meronym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 18 of 27 ***\n",
            "\n",
            "instance_hypernym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 19 of 27 ***\n",
            "\n",
            "member_of_usage_domain lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 20 of 27 ***\n",
            "\n",
            "instances_hyponym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 21 of 27 ***\n",
            "\n",
            "member_of_category_domain lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 22 of 27 ***\n",
            "\n",
            "also_see lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 23 of 27 ***\n",
            "\n",
            "part_holonym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 24 of 27 ***\n",
            "\n",
            "usage_domain lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 25 of 27 ***\n",
            "\n",
            "member_of_region_domain lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 26 of 27 ***\n",
            "\n",
            "member_meronym lem2lem not in rel_name filter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kwWRSH6ThXT",
        "colab_type": "text"
      },
      "source": [
        "`learn_results` is a list of trained models along with model metadata and evaluation results gathered during training and validation. It's a good idea to write these to disk:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FILYl2h-TOue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for lr in learn_results:\n",
        "  learn.store_learn_result(odir, lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCNbaYocQg2N",
        "colab_type": "text"
      },
      "source": [
        "The previous step will have generated a directory structure in the specified output dir. The structure is as follows:\n",
        "\n",
        "    `(odir)/(rel_type)/(rel_name)/(emb_id)/(arch_id)/run_(number)/`\n",
        "\n",
        "See the [embrela README](https://github.com/rdenaux/embrelassess) for more details about the generated files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lejP09cyVXuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar czf nb-pre-trained_models.tar.gz experiment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fayvfNsRLlz",
        "colab_type": "text"
      },
      "source": [
        "## Analysing model results\n",
        "Now that we have trained (and evaluated) models for the selected datasets, we can load the results and analyse them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKosWJkXRmn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import embrelassess.analyse as analyse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqFmCEEOSKDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_read = learn.load_learn_results(odir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afpkbmlxUHaS",
        "colab_type": "text"
      },
      "source": [
        "We'll read the results from disk, aggregate the results and put them into a pandas DataFrame for easier analysis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSgJHuqGUAOw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ea73bbda-92f9-4d4e-e7fd-ab2671a144dc"
      },
      "source": [
        "import pandas as pd\n",
        "aggs = []\n",
        "for learn_result in lr_read:\n",
        "  rel_aggs = analyse.aggregate_runs(learn_result)\n",
        "  print('found', len(rel_aggs), 'for relation', learn_result['rel_name'])\n",
        "  aggs = aggs + rel_aggs\n",
        "        \n",
        "aggs_df = pd.DataFrame(aggs)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "found 4 for relation antonym\n",
            "found 4 for relation entailment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHeXqSKRm9m9",
        "colab_type": "text"
      },
      "source": [
        "We can inspect the resulting DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM3HphBgVDxM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "29215102-6113-4a84-9e93-1514bdecc45d"
      },
      "source": [
        "print(aggs_df.shape)\n",
        "aggs_df"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rel_type</th>\n",
              "      <th>rel_name</th>\n",
              "      <th>emb</th>\n",
              "      <th>acc_avg</th>\n",
              "      <th>acc_std</th>\n",
              "      <th>acc_min</th>\n",
              "      <th>acc_max</th>\n",
              "      <th>f1_avg</th>\n",
              "      <th>f1_std</th>\n",
              "      <th>f1_min</th>\n",
              "      <th>f1_max</th>\n",
              "      <th>precision_avg</th>\n",
              "      <th>precision_std</th>\n",
              "      <th>precision_min</th>\n",
              "      <th>precision_max</th>\n",
              "      <th>recall_avg</th>\n",
              "      <th>recall_std</th>\n",
              "      <th>recall_min</th>\n",
              "      <th>recall_max</th>\n",
              "      <th>model</th>\n",
              "      <th>datapoints</th>\n",
              "      <th>result_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>antonym</td>\n",
              "      <td>holE_wnet_en</td>\n",
              "      <td>0.497139</td>\n",
              "      <td>0.008093</td>\n",
              "      <td>0.491416</td>\n",
              "      <td>0.508584</td>\n",
              "      <td>0.439329</td>\n",
              "      <td>0.310652</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.658993</td>\n",
              "      <td>0.327611</td>\n",
              "      <td>0.231656</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.491416</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.471405</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>antonym</td>\n",
              "      <td>holE_wnet_en</td>\n",
              "      <td>0.501431</td>\n",
              "      <td>0.008280</td>\n",
              "      <td>0.493562</td>\n",
              "      <td>0.512876</td>\n",
              "      <td>0.492722</td>\n",
              "      <td>0.008425</td>\n",
              "      <td>0.484716</td>\n",
              "      <td>0.504367</td>\n",
              "      <td>0.492722</td>\n",
              "      <td>0.008425</td>\n",
              "      <td>0.484716</td>\n",
              "      <td>0.504367</td>\n",
              "      <td>0.492722</td>\n",
              "      <td>0.008425</td>\n",
              "      <td>0.484716</td>\n",
              "      <td>0.504367</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>antonym</td>\n",
              "      <td>rand_en</td>\n",
              "      <td>0.512160</td>\n",
              "      <td>0.003647</td>\n",
              "      <td>0.508584</td>\n",
              "      <td>0.517167</td>\n",
              "      <td>0.482729</td>\n",
              "      <td>0.009687</td>\n",
              "      <td>0.472222</td>\n",
              "      <td>0.495595</td>\n",
              "      <td>0.504089</td>\n",
              "      <td>0.004164</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.509804</td>\n",
              "      <td>0.463610</td>\n",
              "      <td>0.019878</td>\n",
              "      <td>0.445415</td>\n",
              "      <td>0.491266</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>antonym</td>\n",
              "      <td>rand_en</td>\n",
              "      <td>0.482117</td>\n",
              "      <td>0.019379</td>\n",
              "      <td>0.457082</td>\n",
              "      <td>0.504292</td>\n",
              "      <td>0.473071</td>\n",
              "      <td>0.019718</td>\n",
              "      <td>0.447598</td>\n",
              "      <td>0.495633</td>\n",
              "      <td>0.473071</td>\n",
              "      <td>0.019718</td>\n",
              "      <td>0.447598</td>\n",
              "      <td>0.495633</td>\n",
              "      <td>0.473071</td>\n",
              "      <td>0.019718</td>\n",
              "      <td>0.447598</td>\n",
              "      <td>0.495633</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>entailment</td>\n",
              "      <td>holE_wnet_en</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>0.663755</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.663755</td>\n",
              "      <td>0.663755</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>entailment</td>\n",
              "      <td>holE_wnet_en</td>\n",
              "      <td>0.516340</td>\n",
              "      <td>0.010673</td>\n",
              "      <td>0.503268</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.011034</td>\n",
              "      <td>0.486486</td>\n",
              "      <td>0.513513</td>\n",
              "      <td>0.513889</td>\n",
              "      <td>0.011340</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.527778</td>\n",
              "      <td>0.486842</td>\n",
              "      <td>0.010743</td>\n",
              "      <td>0.473684</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>entailment</td>\n",
              "      <td>rand_en</td>\n",
              "      <td>0.625272</td>\n",
              "      <td>0.008152</td>\n",
              "      <td>0.614379</td>\n",
              "      <td>0.633987</td>\n",
              "      <td>0.599856</td>\n",
              "      <td>0.013051</td>\n",
              "      <td>0.581560</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.638503</td>\n",
              "      <td>0.006676</td>\n",
              "      <td>0.630769</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.565789</td>\n",
              "      <td>0.018608</td>\n",
              "      <td>0.539474</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>entailment</td>\n",
              "      <td>rand_en</td>\n",
              "      <td>0.481481</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.450980</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>0.463964</td>\n",
              "      <td>0.035469</td>\n",
              "      <td>0.432432</td>\n",
              "      <td>0.513513</td>\n",
              "      <td>0.476852</td>\n",
              "      <td>0.036454</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.527778</td>\n",
              "      <td>0.451754</td>\n",
              "      <td>0.034535</td>\n",
              "      <td>0.421053</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  rel_type    rel_name           emb  ...  model  datapoints  result_type\n",
              "0  lem2lem     antonym  holE_wnet_en  ...    nn3           3         test\n",
              "1  lem2lem     antonym  holE_wnet_en  ...    nn3           3       random\n",
              "2  lem2lem     antonym       rand_en  ...    nn3           3         test\n",
              "3  lem2lem     antonym       rand_en  ...    nn3           3       random\n",
              "4  lem2lem  entailment  holE_wnet_en  ...    nn3           3         test\n",
              "5  lem2lem  entailment  holE_wnet_en  ...    nn3           3       random\n",
              "6  lem2lem  entailment       rand_en  ...    nn3           3         test\n",
              "7  lem2lem  entailment       rand_en  ...    nn3           3       random\n",
              "\n",
              "[8 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcXiYxhJnIzc",
        "colab_type": "text"
      },
      "source": [
        "As we can see the DataFrame contains 22 columns, including `rel_type`, `rel_name`, `emb` and `model`, which identify the model that was trained and on which relation dataset. \n",
        "\n",
        "You will notice that for each combination we have two sets of results, but they have a different value for column `result_type`:\n",
        "* value `test` indicates results after training\n",
        "* value `random` indicates baseline results when testing using a heuristic choosing a label at random.\n",
        "\n",
        "As you can see, the training in this case does not produce very interesting results since there is no overlap between the words in the datasets and the HolE embeddings. To illustrate the analysis of results, we include results obtained by training on the same datasets using various embeddings. Below, we load the results into a DataFrame and continue analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaRxaBr0EqZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "wnet_results_df = pd.read_csv('embrela_prj/eval_data/aggregated_wn_results.tsv')\n",
        "rand_ds_results_df = pd.read_csv('embrela_prj/eval_data/aggregated_random_dataset_results.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgLxGM_xPeCb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04496cc1-954a-497b-9c63-0355964c7c55"
      },
      "source": [
        "wnet_results_df.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(536, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nJZn5yWPglg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9020aafa-6e54-4a7d-ffe6-c49ff4f3a501"
      },
      "source": [
        "rand_ds_results_df.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(252, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW_3YViMPkOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df = pd.concat([wnet_results_df, rand_ds_results_df])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paiIUOI2PyE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23cb547e-3fc8-443f-9e5a-fd2f867c3fd9"
      },
      "source": [
        "aggs_df.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(788, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHuAHyo9FGF9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "e5f364ef-88b4-4272-af08-4bcf9d5044eb"
      },
      "source": [
        "aggs_df.sample(n=5)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>acc_avg</th>\n",
              "      <th>acc_max</th>\n",
              "      <th>acc_min</th>\n",
              "      <th>acc_std</th>\n",
              "      <th>datapoints</th>\n",
              "      <th>emb</th>\n",
              "      <th>f1_avg</th>\n",
              "      <th>f1_max</th>\n",
              "      <th>f1_min</th>\n",
              "      <th>f1_std</th>\n",
              "      <th>model</th>\n",
              "      <th>precision_avg</th>\n",
              "      <th>precision_max</th>\n",
              "      <th>precision_min</th>\n",
              "      <th>precision_std</th>\n",
              "      <th>recall_avg</th>\n",
              "      <th>recall_max</th>\n",
              "      <th>recall_min</th>\n",
              "      <th>recall_std</th>\n",
              "      <th>rel_name</th>\n",
              "      <th>rel_type</th>\n",
              "      <th>result_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>410</td>\n",
              "      <td>0.497348</td>\n",
              "      <td>0.524181</td>\n",
              "      <td>0.455538</td>\n",
              "      <td>0.023647</td>\n",
              "      <td>5</td>\n",
              "      <td>ft_wiki_en</td>\n",
              "      <td>0.503544</td>\n",
              "      <td>0.530046</td>\n",
              "      <td>0.462250</td>\n",
              "      <td>0.023356</td>\n",
              "      <td>nn3</td>\n",
              "      <td>0.483432</td>\n",
              "      <td>0.508876</td>\n",
              "      <td>0.443787</td>\n",
              "      <td>0.022423</td>\n",
              "      <td>0.525402</td>\n",
              "      <td>0.553055</td>\n",
              "      <td>0.482315</td>\n",
              "      <td>0.024370</td>\n",
              "      <td>part_meronym</td>\n",
              "      <td>lem2lem</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>63</td>\n",
              "      <td>0.505202</td>\n",
              "      <td>0.560694</td>\n",
              "      <td>0.433526</td>\n",
              "      <td>0.044715</td>\n",
              "      <td>5</td>\n",
              "      <td>glove_cc_en</td>\n",
              "      <td>0.524444</td>\n",
              "      <td>0.577778</td>\n",
              "      <td>0.455556</td>\n",
              "      <td>0.042976</td>\n",
              "      <td>nn2</td>\n",
              "      <td>0.507527</td>\n",
              "      <td>0.559140</td>\n",
              "      <td>0.440860</td>\n",
              "      <td>0.041589</td>\n",
              "      <td>0.542529</td>\n",
              "      <td>0.597701</td>\n",
              "      <td>0.471264</td>\n",
              "      <td>0.044458</td>\n",
              "      <td>attribute</td>\n",
              "      <td>lem2lem</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>264</td>\n",
              "      <td>0.424667</td>\n",
              "      <td>0.444000</td>\n",
              "      <td>0.388000</td>\n",
              "      <td>0.025940</td>\n",
              "      <td>3</td>\n",
              "      <td>swivelsyn_wiki_en_8e</td>\n",
              "      <td>0.487095</td>\n",
              "      <td>0.510563</td>\n",
              "      <td>0.462428</td>\n",
              "      <td>0.019670</td>\n",
              "      <td>nn3</td>\n",
              "      <td>0.417472</td>\n",
              "      <td>0.432836</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.013488</td>\n",
              "      <td>0.587983</td>\n",
              "      <td>0.626609</td>\n",
              "      <td>0.515021</td>\n",
              "      <td>0.051621</td>\n",
              "      <td>random_5000</td>\n",
              "      <td>syn2syn</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>362</td>\n",
              "      <td>0.497074</td>\n",
              "      <td>0.530120</td>\n",
              "      <td>0.457831</td>\n",
              "      <td>0.026718</td>\n",
              "      <td>5</td>\n",
              "      <td>glove_cc_en</td>\n",
              "      <td>0.484656</td>\n",
              "      <td>0.518519</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.027377</td>\n",
              "      <td>nn3</td>\n",
              "      <td>0.477083</td>\n",
              "      <td>0.510417</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.026949</td>\n",
              "      <td>0.492473</td>\n",
              "      <td>0.526882</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.027819</td>\n",
              "      <td>also_see</td>\n",
              "      <td>lem2lem</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>214</td>\n",
              "      <td>0.503862</td>\n",
              "      <td>0.512361</td>\n",
              "      <td>0.494721</td>\n",
              "      <td>0.007500</td>\n",
              "      <td>5</td>\n",
              "      <td>vecsi_UN_en</td>\n",
              "      <td>0.508069</td>\n",
              "      <td>0.516497</td>\n",
              "      <td>0.499006</td>\n",
              "      <td>0.007436</td>\n",
              "      <td>nn3</td>\n",
              "      <td>0.506526</td>\n",
              "      <td>0.514927</td>\n",
              "      <td>0.497490</td>\n",
              "      <td>0.007413</td>\n",
              "      <td>0.509623</td>\n",
              "      <td>0.518075</td>\n",
              "      <td>0.500532</td>\n",
              "      <td>0.007459</td>\n",
              "      <td>synonym</td>\n",
              "      <td>lem2lem</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0   acc_avg   acc_max  ...      rel_name  rel_type  result_type\n",
              "410         410  0.497348  0.524181  ...  part_meronym   lem2lem       random\n",
              "63           63  0.505202  0.560694  ...     attribute   lem2lem       random\n",
              "196         264  0.424667  0.444000  ...   random_5000   syn2syn         test\n",
              "362         362  0.497074  0.530120  ...      also_see   lem2lem       random\n",
              "214         214  0.503862  0.512361  ...       synonym   lem2lem       random\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUrJEmRNFPex",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the data read from the tsv has the same columns as we would get by aggregating the results read from disk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0PNWokSFrqW",
        "colab_type": "text"
      },
      "source": [
        "### Combine fields and define filters\n",
        "Most of the columns in the DataFrame are aggregated values, but for further analysis it's useful to combine fields and relation metadata.\n",
        "\n",
        "First we add column `rel_name_type`, which combines the `rel_name` and `rel_type`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m24QXudXFlN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df['rel_name_type'] = aggs_df.apply(lambda row: '%s_%s' % (row['rel_name'], row['rel_type']), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ7z0e2mGkuW",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to fix some terminology issues. We rename column `rel_type` to `pair_type` because values like `lem2lem` describes the types of pairs we tried to predict. We use `rel_type` to create high-level relation types like `similarity` and `lexical`,  as described in the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dus7KSW1GtuA",
        "colab_type": "text"
      },
      "source": [
        "We'll add a column 'KG' to document that all the relations came from WordNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfgTa2JfG8R9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df['KG'] = 'wnet'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcyIidhVGiPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df.rename(columns={'rel_type': 'pair_type'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg_eFwCKIOrC",
        "colab_type": "text"
      },
      "source": [
        "The `embrela` project defines a utility script `relutil` which calculates the relation type based on the rows in the DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrDOrvxdHu-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp embrela_prj/relutil.py ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIQXStN4H-jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import relutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5zxdKytICIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df['rel_type'] = aggs_df.apply(\n",
        "    relutil.calc_rel_type_for_dfrow_fn(rel_name_field='rel_name'), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hWzzVzmIqst",
        "colab_type": "text"
      },
      "source": [
        "We also add column `emb_corpus_type` to distinguish between types of embeddings used:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB6O21sAI4CM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_emb_corpus_type(row):\n",
        "    emb=row['emb']\n",
        "    if emb.startswith('holE_'):\n",
        "        return 'kg'\n",
        "    elif emb.startswith('rand_'):\n",
        "        return 'no-corpus'\n",
        "    elif emb.startswith('swivelsyn_'):\n",
        "        return 'concept-corpus'\n",
        "    elif emb.startswith('ftsyn_'):\n",
        "        return 'concept-corpus'\n",
        "    elif emb.startswith('vecsi_'):\n",
        "        return 'word-concept-corpus'\n",
        "    elif emb.startswith('sw2v_umbc'):\n",
        "        return 'word-corpus'\n",
        "    else:\n",
        "        return 'word-corpus'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEjs9KdhI6So",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df['emb_corpus_type'] = aggs_df.apply(calc_emb_corpus_type, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHi7D95OJJMr",
        "colab_type": "text"
      },
      "source": [
        "Now we start merging the relation dataset metadata into the aggregated DataFrame. We first add a field `positive_examples`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht-QClO9JTqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_positive_examples(row):\n",
        "    name = row['rel_name']\n",
        "    ptype = row['pair_type']\n",
        "    \n",
        "    name_filter = rels_df['name'] == name\n",
        "    ptype_filter = rels_df['type'] == ptype\n",
        "    cnts = rels_df[name_filter & ptype_filter]['cnt'].values\n",
        "    if len(cnts) > 0:\n",
        "      return cnts[0]\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0n5PU6NJoP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df['positive_examples'] = aggs_df.apply(calc_positive_examples, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0cyNddrJ2By",
        "colab_type": "text"
      },
      "source": [
        "### Calculate the range thresholds\n",
        "Next, we can calculate range thresholds as described in the paper.  $\\tau^{\\upsilon_\\mathrm{min}}_{\\mathrm{biased}} = \\mu^\\upsilon_{\\delta_{\\mathtt{rand},x}} - 2 \\sigma^\\upsilon_{\\delta{\\mathrm{rand},x}}$ and $\\tau^{\\upsilon_\\mathrm{max}}_{\\mathrm{biased}} = \\mu^\\upsilon_{\\delta_{\\mathtt{rand},x}} + 2 \\sigma^\\upsilon_{\\delta{\\mathrm{rand},x}}$. Any metrics within these ranges could be due to chance with 95\\% probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbrtR_J9K9Mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_rand_avg_and_std(metric='f1', debug=False):\n",
        "    randomrel_filter = aggs_df['rel_name'].str.startswith('random_')\n",
        "    test_result_filter = aggs_df['result_type']=='test'\n",
        "\n",
        "    avg_field = '%s_avg' % metric\n",
        "    std_field = '%s_std' % metric\n",
        "    \n",
        "    sub_df = aggs_df[randomrel_filter & test_result_filter]\n",
        "    if debug:\n",
        "        print('calculating averages over %d measurements' % sub_df.shape[0])\n",
        "        print('rels', sub_df['rel_name'].unique())\n",
        "        print('embs', sub_df['emb'].unique())\n",
        "        sub_df[avg_field].hist()\n",
        "    rand_metric_avg2 = sub_df[avg_field].mean()\n",
        "    rand_metric_avg_std = sub_df[avg_field].std()\n",
        "    rand_metric_std_avg = sub_df[std_field].mean()\n",
        "    max_std = max(rand_metric_avg_std, rand_metric_std_avg)\n",
        "    \n",
        "    return {'avg': rand_metric_avg2, 'std': max_std}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqlt-boaLkzO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "e07ac3db-dc94-4195-b25a-b0ed1a6ca97f"
      },
      "source": [
        "calc_rand_avg_and_std(debug=True)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "calculating averages over 126 measurements\n",
            "rels ['random_1000' 'random_10000' 'random_5000' 'random_200' 'random_500'\n",
            " 'random_50000']\n",
            "embs ['glove_un_en' 'swivel_wiki_en' 'holE_sensi_en' 'ft_un_en' 'ft_cc_en'\n",
            " 'vecsi_un_en' 'glove_wiki_en' 'glove_cc_en' 'ft_wiki_en' 'vecsi_wiki_en'\n",
            " 'rand_en' 'swivel_un_en' 'vecsi_un_en_ts' 'swivelsyn_wiki_en_25e'\n",
            " 'ftsyn_wiki' 'swivelsyn_wiki_en_8e']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'avg': 0.4070040067097273, 'std': 0.12301846272363581}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSgRX2M6SEcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}