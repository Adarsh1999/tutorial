{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04a_assessing_relational_knowledge",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vkl7mxsBpn6E",
        "colab_type": "text"
      },
      "source": [
        "# Assessing Relational Knowledge captured by embeddings\n",
        "\n",
        "In this notebook, we use the `embrela` library to study  whether various embedding spaces capture certain lexico-semantic relations on WordNet. The approach behind `embrela` is described in:\n",
        "\n",
        "Denaux, R. and Gomez-Perez, J.M., 2019, September.\n",
        "*Assessing the Lexico-Semantic Relational Knowledge Captured by Word and Concept Embeddings.* In Proceedings of the 10th International Conference on Knowledge Capture (pp. 29-36). ACM. [preprint](https://arxiv.org/abs/1909.11042)\n",
        "\n",
        "The main idea here is that word/concept embeddings seem to capture many lexico-semantic relations. However evaluation methods like word similarity and word analogy have several drawbacks. Also, if you have an existing KG with relations that matter to you, you want to know how well word/concept embeddings capture those relations. The `embrela` pipeline is designed to help you do this by:\n",
        "1. generating *word/concept pair datasets* from a KG, \n",
        "2. creating and evaluating classifiers based on the embedding space(s) \n",
        "3. providing guidelines on how to analyse the evaluation results\n",
        "\n",
        "There are **pitfalls to all three of these steps**, which the `embrela` pipeline takes into account in order to *avoid concluding incorrectly that embeddings capture relational information* when in fact, the generated dataset may be biased or evaluation results may not be statistically significantly better than random guesses. The overall pipeline looks as follows:\n",
        "\n",
        "![embrela pipeline](https://github.com/rdenaux/embrelassess/raw/master/wnet-rel-pair-extractor/src/embrel-assess-pipeline.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKhhkYugrdnm",
        "colab_type": "text"
      },
      "source": [
        "### Download `embrela` project\n",
        "We put the main python module on the main working folder to make importing of submodules a bit easier to read.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bmXQvaowNYm",
        "colab_type": "code",
        "outputId": "189d4323-faf3-45a7-cad1-df9a747a70a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "!git clone https://github.com/rdenaux/embrelassess.git embrela_prj\n",
        "!ln -s embrela_prj/embrelassess embrelassess"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'embrela_prj'...\n",
            "remote: Enumerating objects: 304, done.\u001b[K\n",
            "remote: Counting objects: 100% (304/304), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 304 (delta 138), reused 295 (delta 132), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (304/304), 14.64 MiB | 6.98 MiB/s, done.\n",
            "Resolving deltas: 100% (138/138), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHkfMnDlrRFz",
        "colab_type": "text"
      },
      "source": [
        "## 1. Download generated datasets\n",
        "Instead of generating datasets from scratch, which can be done by using the `embrela_prj/wnet-rel-pair-extractor`, in this notebook we'll use a set of pre-generated datasets extracted from WordNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwAHbFOI3JQN",
        "colab_type": "code",
        "outputId": "3064b44b-34fb-4688-daee-9db5d0a09548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        }
      },
      "source": [
        "!wget https://github.com/rdenaux/embrelassess/releases/download/v0.1/vocabrels_wnet-switched-negs.zip\n",
        "!unzip vocabrels_wnet-switched-negs.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-19 13:19:12--  https://github.com/rdenaux/embrelassess/releases/download/v0.1/vocabrels_wnet-switched-negs.zip\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/210606207/573bc500-092b-11ea-854d-e8b23654ea2e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191119%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191119T131913Z&X-Amz-Expires=300&X-Amz-Signature=835788554f1693db662ede327fa9fbc5cb492146090f1796e96d6fd901da6aeb&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dvocabrels_wnet-switched-negs.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-11-19 13:19:13--  https://github-production-release-asset-2e65be.s3.amazonaws.com/210606207/573bc500-092b-11ea-854d-e8b23654ea2e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191119%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191119T131913Z&X-Amz-Expires=300&X-Amz-Signature=835788554f1693db662ede327fa9fbc5cb492146090f1796e96d6fd901da6aeb&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dvocabrels_wnet-switched-negs.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.101.147\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.101.147|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8844325 (8.4M) [application/octet-stream]\n",
            "Saving to: ‘vocabrels_wnet-switched-negs.zip’\n",
            "\n",
            "vocabrels_wnet-swit 100%[===================>]   8.43M  5.12MB/s    in 1.6s    \n",
            "\n",
            "2019-11-19 13:19:16 (5.12 MB/s) - ‘vocabrels_wnet-switched-negs.zip’ saved [8844325/8844325]\n",
            "\n",
            "Archive:  vocabrels_wnet-switched-negs.zip\n",
            "   creating: vocabrels_wn-switched-negs/\n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_also_see__5800.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_antonym__9310.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_attribute__1718.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_category_domain__9116.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_cause__719.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_derivation__118888.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_entailment__1519.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_hypernym__110650.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_hyponym__110650.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_instances_hyponym__2358.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_instance_hypernym__2358.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_member_holonym__1315.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_member_meronym__1315.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_member_of_category_domain__9116.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_member_of_region_domain__1349.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_member_of_usage_domain__846.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_participle_of__81.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_part_holonym__6403.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_part_meronym__6403.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_pertainym__6516.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_region_domain__1349.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_similar__20464.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_substance_holonym__369.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_substance_meronym__369.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_synonym__74822.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_usage_domain__846.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lem2lem_verb_group__4944.txt  \n",
            "  inflating: vocabrels_wn-switched-negs/lempairs.tsv  \n",
            "  inflating: vocabrels_wn-switched-negs/synpairs.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiq9p3iitAje",
        "colab_type": "text"
      },
      "source": [
        "We can load the metadata of the generated relations as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFgsWWbr4D5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import embrelassess.learn as learn\n",
        "import os.path as osp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcQQ5PBnokZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rel_path = osp.join('vocabrels_wn-switched-negs/')\n",
        "rels_df = learn.load_rels_meta(rel_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOZaWPGWtLZm",
        "colab_type": "text"
      },
      "source": [
        "Which gives us a pandas DataFrame with metadata about the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqPvZcvxpstc",
        "colab_type": "code",
        "outputId": "234ebe79-85ad-4783-a8c1-a101e5605f3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        }
      },
      "source": [
        "rels_df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>name</th>\n",
              "      <th>cnt</th>\n",
              "      <th>file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>member_of_category_domain</td>\n",
              "      <td>9116</td>\n",
              "      <td>lem2lem_member_of_category_domain__9116.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>synonym</td>\n",
              "      <td>74822</td>\n",
              "      <td>lem2lem_synonym__74822.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>entailment</td>\n",
              "      <td>1519</td>\n",
              "      <td>lem2lem_entailment__1519.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>part_meronym</td>\n",
              "      <td>6403</td>\n",
              "      <td>lem2lem_part_meronym__6403.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>region_domain</td>\n",
              "      <td>1349</td>\n",
              "      <td>lem2lem_region_domain__1349.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>instances_hyponym</td>\n",
              "      <td>2358</td>\n",
              "      <td>lem2lem_instances_hyponym__2358.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>member_meronym</td>\n",
              "      <td>1315</td>\n",
              "      <td>lem2lem_member_meronym__1315.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>instance_hypernym</td>\n",
              "      <td>2358</td>\n",
              "      <td>lem2lem_instance_hypernym__2358.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>member_of_region_domain</td>\n",
              "      <td>1349</td>\n",
              "      <td>lem2lem_member_of_region_domain__1349.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>hypernym</td>\n",
              "      <td>110650</td>\n",
              "      <td>lem2lem_hypernym__110650.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>substance_holonym</td>\n",
              "      <td>369</td>\n",
              "      <td>lem2lem_substance_holonym__369.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>hyponym</td>\n",
              "      <td>110650</td>\n",
              "      <td>lem2lem_hyponym__110650.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>cause</td>\n",
              "      <td>719</td>\n",
              "      <td>lem2lem_cause__719.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>similar</td>\n",
              "      <td>20464</td>\n",
              "      <td>lem2lem_similar__20464.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>attribute</td>\n",
              "      <td>1718</td>\n",
              "      <td>lem2lem_attribute__1718.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>usage_domain</td>\n",
              "      <td>846</td>\n",
              "      <td>lem2lem_usage_domain__846.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>pertainym</td>\n",
              "      <td>6516</td>\n",
              "      <td>lem2lem_pertainym__6516.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>substance_meronym</td>\n",
              "      <td>369</td>\n",
              "      <td>lem2lem_substance_meronym__369.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>verb_group</td>\n",
              "      <td>4944</td>\n",
              "      <td>lem2lem_verb_group__4944.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>category_domain</td>\n",
              "      <td>9116</td>\n",
              "      <td>lem2lem_category_domain__9116.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>derivation</td>\n",
              "      <td>118888</td>\n",
              "      <td>lem2lem_derivation__118888.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>antonym</td>\n",
              "      <td>9310</td>\n",
              "      <td>lem2lem_antonym__9310.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>member_of_usage_domain</td>\n",
              "      <td>846</td>\n",
              "      <td>lem2lem_member_of_usage_domain__846.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>member_holonym</td>\n",
              "      <td>1315</td>\n",
              "      <td>lem2lem_member_holonym__1315.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>also_see</td>\n",
              "      <td>5800</td>\n",
              "      <td>lem2lem_also_see__5800.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>part_holonym</td>\n",
              "      <td>6403</td>\n",
              "      <td>lem2lem_part_holonym__6403.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>participle_of</td>\n",
              "      <td>81</td>\n",
              "      <td>lem2lem_participle_of__81.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       type  ...                                         file\n",
              "0   lem2lem  ...  lem2lem_member_of_category_domain__9116.txt\n",
              "1   lem2lem  ...                   lem2lem_synonym__74822.txt\n",
              "2   lem2lem  ...                 lem2lem_entailment__1519.txt\n",
              "3   lem2lem  ...               lem2lem_part_meronym__6403.txt\n",
              "4   lem2lem  ...              lem2lem_region_domain__1349.txt\n",
              "5   lem2lem  ...          lem2lem_instances_hyponym__2358.txt\n",
              "6   lem2lem  ...             lem2lem_member_meronym__1315.txt\n",
              "7   lem2lem  ...          lem2lem_instance_hypernym__2358.txt\n",
              "8   lem2lem  ...    lem2lem_member_of_region_domain__1349.txt\n",
              "9   lem2lem  ...                 lem2lem_hypernym__110650.txt\n",
              "10  lem2lem  ...           lem2lem_substance_holonym__369.txt\n",
              "11  lem2lem  ...                  lem2lem_hyponym__110650.txt\n",
              "12  lem2lem  ...                       lem2lem_cause__719.txt\n",
              "13  lem2lem  ...                   lem2lem_similar__20464.txt\n",
              "14  lem2lem  ...                  lem2lem_attribute__1718.txt\n",
              "15  lem2lem  ...                lem2lem_usage_domain__846.txt\n",
              "16  lem2lem  ...                  lem2lem_pertainym__6516.txt\n",
              "17  lem2lem  ...           lem2lem_substance_meronym__369.txt\n",
              "18  lem2lem  ...                 lem2lem_verb_group__4944.txt\n",
              "19  lem2lem  ...            lem2lem_category_domain__9116.txt\n",
              "20  lem2lem  ...               lem2lem_derivation__118888.txt\n",
              "21  lem2lem  ...                    lem2lem_antonym__9310.txt\n",
              "22  lem2lem  ...      lem2lem_member_of_usage_domain__846.txt\n",
              "23  lem2lem  ...             lem2lem_member_holonym__1315.txt\n",
              "24  lem2lem  ...                   lem2lem_also_see__5800.txt\n",
              "25  lem2lem  ...               lem2lem_part_holonym__6403.txt\n",
              "26  lem2lem  ...                lem2lem_participle_of__81.txt\n",
              "\n",
              "[27 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcBNuJw-tkJ1",
        "colab_type": "text"
      },
      "source": [
        "The previous step should have printed a table with 27 rows. One for each generated dataset. All of the datasets are `lem2lem`, i.e. the *source* and *target*s are both lemmas. In the paper we also consider pairs of types `lem2syn`, `syn2syn` and `lem2pos`, where `syn` is a synset (or syncon) and `pos` is part-of-speech. \n",
        "\n",
        "The remaining columsn in the table tell us:\n",
        "* the `name` of the relation \n",
        "* `cnt`: the number of **positive** examples extracted from the KG, WorNet 3 in this case.\n",
        "* the `file` name where we can find both the positive and negative examples\n",
        "\n",
        "Note that each dataset will have about twice the number of lines as the positive `cnt`, since we aim to build balanced datasets. For example for the `entailment` relation, we have 1519 positive pairs, but:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVQocJqouX7Y",
        "colab_type": "code",
        "outputId": "441756e8-6dc6-4653-ba8b-9960be2e2495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!wc -l vocabrels_wn-switched-negs/lem2lem_entailment__1519.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3039 vocabrels_wn-switched-negs/lem2lem_entailment__1519.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_nBGsqSvTy8",
        "colab_type": "text"
      },
      "source": [
        "3039 lines in total. Further inspection of the file shows that it's a tab-separated-value with columns:\n",
        "`source`, `target`, `label` and `comment`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMc4BvCvwAR0",
        "colab_type": "code",
        "outputId": "054db859-ad71-4de6-8159-1cc5c791e0b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!head -n 5 vocabrels_wn-switched-negs/lem2lem_antonym__9310.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "specialize\tdiversify\t1\tpositive\n",
            "give off\taffirm\t0\t[NegSwitched]\n",
            "beginning\tending\t1\tpositive\n",
            "take off\tdc\t0\t[NegSwitched]\n",
            "hot\tcold\t1\tpositive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l53Bw-KLqboP",
        "colab_type": "text"
      },
      "source": [
        "## 2. Load the embeddings to be evaluated\n",
        "\n",
        "We will use the following embeddings:\n",
        "* wordnet embeddings trained using **HolE**, which learns embeddings directly from the WordNet graph. This is a strong baseline\n",
        "* **FastText**\n",
        "* **GloVe**\n",
        "\n",
        "First, we download the pre-trained HolE embeddings. These have been trained for 500 epochs and have 150 dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b59_Y7cG3b9l",
        "colab_type": "code",
        "outputId": "7b0c4c92-862e-4312-c076-e0c989f41bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!wget https://github.com/rdenaux/embrelassess/releases/download/v0.1.1/wn-en-3.1.-HolE-500e-150d.vec.tar.gz\n",
        "!tar xzf wn-en-3.1.-HolE-500e-150d.vec.tar.gz"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-19 13:20:47--  https://github.com/rdenaux/embrelassess/releases/download/v0.1.1/wn-en-3.1.-HolE-500e-150d.vec.tar.gz\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/210606207/46915a80-09d0-11ea-91aa-ac4fb6373635?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191119%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191119T132048Z&X-Amz-Expires=300&X-Amz-Signature=c2ef132bbd16de183d5c61381677b503ccbe85f84ef4973a2fe0f594f6f2ffc3&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dwn-en-3.1.-HolE-500e-150d.vec.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-11-19 13:20:48--  https://github-production-release-asset-2e65be.s3.amazonaws.com/210606207/46915a80-09d0-11ea-91aa-ac4fb6373635?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191119%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191119T132048Z&X-Amz-Expires=300&X-Amz-Signature=c2ef132bbd16de183d5c61381677b503ccbe85f84ef4973a2fe0f594f6f2ffc3&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dwn-en-3.1.-HolE-500e-150d.vec.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.233.203\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.233.203|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 369436200 (352M) [application/octet-stream]\n",
            "Saving to: ‘wn-en-3.1.-HolE-500e-150d.vec.tar.gz’\n",
            "\n",
            "wn-en-3.1.-HolE-500 100%[===================>] 352.32M  13.1MB/s    in 28s     \n",
            "\n",
            "2019-11-19 13:21:17 (12.6 MB/s) - ‘wn-en-3.1.-HolE-500e-150d.vec.tar.gz’ saved [369436200/369436200]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cDxGIwlxlbc",
        "colab_type": "text"
      },
      "source": [
        "Next, we can load the embeddings. The `embrela` library, as well as `torchtext` has various convenience methods to do this. `torchtext` automatically downloads pre-trained embeddings published at the official FastText and Glove sites."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Ni566hx7ZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import embrelassess.embloader as embloader\n",
        "#import torchtext\n",
        "from torchtext.vocab import Vectors, FastText, GloVe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz0VtxF9yQw9",
        "colab_type": "text"
      },
      "source": [
        "`torchtext` [defines a number of `aliases`](https://torchtext.readthedocs.io/en/latest/vocab.html#pretrained-aliases) for pre-trained embeddings. We use:\n",
        "* the `glove.6B.100d` pre-trained embeddings. In the paper we've used the `glove.840B.300d`, but these take longer to download\n",
        "* the `fasttext.simple.300d` embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeehWpy7zRWp",
        "colab_type": "code",
        "outputId": "f95a6fd8-f8bd-4355-a9b7-5041d12a700b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# uncomment next line if you want to use GloVe embeddings. These can take a while to load\n",
        "# glove_en = GloVe(name='6B', dim=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                           \n",
            "100%|█████████▉| 399723/400000 [00:15<00:00, 25505.80it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkGCCl9z5ObL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uncomment if you want to use FastText. These can take a while to load\n",
        "FastText.url_base = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.{}.vec'\n",
        "#ft_en = FastText(language='en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfLq8OVtyBde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "holE_wnet_en = embloader.TSVVectors('wn-en-3.1-HolE-500e.vec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO8vjDYNwnAa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4922256-1b74-4bbf-9451-1cd222d7c84b"
      },
      "source": [
        "len(list(holE_wnet_en.stoi.keys()))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "264965"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KksjZyYhJhUU",
        "colab_type": "text"
      },
      "source": [
        "We will also use random vectors as another baseline and to filter out biased relations. We use the vocabulary of words and syncons used in the K-CAP'19 paper, which was derived from disambiguating the UN corpus using Cogito."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvH0yMzfEsSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_path = osp.join('embrela_prj/vocab_sensi_lemsyn_UN.txt')\n",
        "rnd_vecs = embloader.RandomVectors(vocab_path, dim=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnrv8-uUJ5ep",
        "colab_type": "text"
      },
      "source": [
        "The training phase expects to get a map of vector space ids to `VecPairLoader` instances, which will take care of mapping `source` and `target` words in the generated datasets into the appropriate embeddings. Here we define the data loaders to use. Uncomment others if you want to use other embedding spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvtvsACpCvk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_loaders = {\n",
        "        #'glove_cc_en':    embloader.VecPairLoader(glove_en),\n",
        "        #'ft_wikip_en':    embloader.VecPairLoader(ft_en),\n",
        "        #'vecsi_wiki_en': embloader.VecPairLoader(vecsi_wiki_en),\n",
        "        #'vecsi_un_en':   embloader.VecPairLoader(vecsi_un_en),\n",
        "        'rand_en':        embloader.VecPairLoader(rnd_vecs),\n",
        "        'holE_wnet_en':   embloader.VecPairLoader(holE_wnet_en)\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8gKLRpjKW3A",
        "colab_type": "text"
      },
      "source": [
        "## Learn models\n",
        "Now that we have the datasets and the embeddings, we are ready to train some models. This step is highly configurable, but in this notebook we'll:\n",
        "* only train models with the `nn3` architecture (ie with 3 fully connected layers)\n",
        "* only train models for a couple of (the 27) relations to keep execution short\n",
        "* only train 3 models per embedding/relation/architecture combination\n",
        "* apply *input perturbation* as explained in the paper which shifts both `source` and `target` embeddings by the same amount\n",
        "\n",
        "The trained models and evaluation results will be written to an output folder. Even with this restricted setup, this step can take 5 to 10 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka7xvBENLD_S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4c3c33c-49aa-4bb1-ff74-ef6eba21d7d8"
      },
      "source": [
        "model_archs = ['nn3']\n",
        "n_runs = 3\n",
        "my_rels = ['entailment', 'antonym']\n",
        "def only_with_names(relname_whitelist):\n",
        "  return lambda df_row: df_row['name'] in relname_whitelist\n",
        "\n",
        "odir = 'experiment/trained_models/'\n",
        "\n",
        "learn_results = learn.learn_rels(\n",
        "    rel_path, rels_df, data_loaders,\n",
        "    models=model_archs, n_runs=n_runs,\n",
        "    rel_filter=only_with_names(my_rels),\n",
        "    train_input_disturber_for_vec=learn.pair_disturber_for_vectors,\n",
        "    odir_path=odir, cuda=True)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 4.77 µs\n",
            "\n",
            "*** rel 0 of 27 ***\n",
            "\n",
            "region_domain lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 1 of 27 ***\n",
            "\n",
            "cause lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 2 of 27 ***\n",
            "\n",
            "Training each model 3 times...\n",
            "run 0 on model nn3 with vectors rand_en\n",
            "\n",
            " lem2lem_antonym__9310.txt\n",
            "train (16758,), validate (931,), test (932,)\n",
            "fcs 4, dropouts 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NNBiClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished 12 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 932, 'threshold': 1.0, 'examples_above_threshold': 932, 'correct': 452, 'tp': 218, 'fp': 240, 'fn': 240}\n",
            "For randpredict on 932(100.00%) of 932 examples > 1.00 confidence, prec, rec, acc, f1: 47 47 48 47 %\n",
            "run 1 on model nn3 with vectors rand_en\n",
            "\n",
            " lem2lem_antonym__9310.txt\n",
            "train (16758,), validate (931,), test (932,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 12 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 932, 'threshold': 1.0, 'examples_above_threshold': 932, 'correct': 426, 'tp': 205, 'fp': 253, 'fn': 253}\n",
            "For randpredict on 932(100.00%) of 932 examples > 1.00 confidence, prec, rec, acc, f1: 44 44 45 44 %\n",
            "run 2 on model nn3 with vectors rand_en\n",
            "\n",
            " lem2lem_antonym__9310.txt\n",
            "train (16758,), validate (931,), test (932,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 12 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 932, 'threshold': 1.0, 'examples_above_threshold': 932, 'correct': 470, 'tp': 227, 'fp': 231, 'fn': 231}\n",
            "For randpredict on 932(100.00%) of 932 examples > 1.00 confidence, prec, rec, acc, f1: 49 49 50 49 %\n",
            "run 0 on model nn3 with vectors holE_wnet_en\n",
            "\n",
            " lem2lem_antonym__9310.txt\n",
            "train (16758,), validate (931,), test (932,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 12 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 932, 'threshold': 1.0, 'examples_above_threshold': 932, 'correct': 478, 'tp': 231, 'fp': 227, 'fn': 227}\n",
            "For randpredict on 932(100.00%) of 932 examples > 1.00 confidence, prec, rec, acc, f1: 50 50 51 50 %\n",
            "run 1 on model nn3 with vectors holE_wnet_en\n",
            "\n",
            " lem2lem_antonym__9310.txt\n",
            "train (16758,), validate (931,), test (932,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 12 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 932, 'threshold': 1.0, 'examples_above_threshold': 932, 'correct': 464, 'tp': 224, 'fp': 234, 'fn': 234}\n",
            "For randpredict on 932(100.00%) of 932 examples > 1.00 confidence, prec, rec, acc, f1: 48 48 49 48 %\n",
            "run 2 on model nn3 with vectors holE_wnet_en\n",
            "\n",
            " lem2lem_antonym__9310.txt\n",
            "train (16758,), validate (931,), test (932,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 12 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 932, 'threshold': 1.0, 'examples_above_threshold': 932, 'correct': 460, 'tp': 222, 'fp': 236, 'fn': 236}\n",
            "For randpredict on 932(100.00%) of 932 examples > 1.00 confidence, prec, rec, acc, f1: 48 48 49 48 %\n",
            "\n",
            "*** rel 3 of 27 ***\n",
            "\n",
            "similar lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 4 of 27 ***\n",
            "\n",
            "verb_group lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 5 of 27 ***\n",
            "\n",
            "substance_holonym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 6 of 27 ***\n",
            "\n",
            "derivation lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 7 of 27 ***\n",
            "\n",
            "synonym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 8 of 27 ***\n",
            "\n",
            "pertainym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 9 of 27 ***\n",
            "\n",
            "participle_of lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 10 of 27 ***\n",
            "\n",
            "attribute lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 11 of 27 ***\n",
            "\n",
            "part_meronym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 12 of 27 ***\n",
            "\n",
            "hypernym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 13 of 27 ***\n",
            "\n",
            "member_holonym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 14 of 27 ***\n",
            "\n",
            "Training each model 3 times...\n",
            "run 0 on model nn3 with vectors rand_en\n",
            "\n",
            " lem2lem_entailment__1519.txt\n",
            "train (2735,), validate (151,), test (153,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 24 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 153, 'threshold': 1.0, 'examples_above_threshold': 153, 'correct': 69, 'tp': 32, 'fp': 40, 'fn': 44}\n",
            "For randpredict on 153(100.00%) of 153 examples > 1.00 confidence, prec, rec, acc, f1: 44 42 45 43 %\n",
            "run 1 on model nn3 with vectors rand_en\n",
            "\n",
            " lem2lem_entailment__1519.txt\n",
            "train (2735,), validate (151,), test (153,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 24 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 153, 'threshold': 1.0, 'examples_above_threshold': 153, 'correct': 81, 'tp': 38, 'fp': 34, 'fn': 38}\n",
            "For randpredict on 153(100.00%) of 153 examples > 1.00 confidence, prec, rec, acc, f1: 52 50 52 51 %\n",
            "run 2 on model nn3 with vectors rand_en\n",
            "\n",
            " lem2lem_entailment__1519.txt\n",
            "train (2735,), validate (151,), test (153,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 24 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 153, 'threshold': 1.0, 'examples_above_threshold': 153, 'correct': 71, 'tp': 33, 'fp': 39, 'fn': 43}\n",
            "For randpredict on 153(100.00%) of 153 examples > 1.00 confidence, prec, rec, acc, f1: 45 43 46 44 %\n",
            "run 0 on model nn3 with vectors holE_wnet_en\n",
            "\n",
            " lem2lem_entailment__1519.txt\n",
            "train (2735,), validate (151,), test (153,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 24 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 153, 'threshold': 1.0, 'examples_above_threshold': 153, 'correct': 77, 'tp': 36, 'fp': 36, 'fn': 40}\n",
            "For randpredict on 153(100.00%) of 153 examples > 1.00 confidence, prec, rec, acc, f1: 50 47 50 48 %\n",
            "run 1 on model nn3 with vectors holE_wnet_en\n",
            "\n",
            " lem2lem_entailment__1519.txt\n",
            "train (2735,), validate (151,), test (153,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 24 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 153, 'threshold': 1.0, 'examples_above_threshold': 153, 'correct': 81, 'tp': 38, 'fp': 34, 'fn': 38}\n",
            "For randpredict on 153(100.00%) of 153 examples > 1.00 confidence, prec, rec, acc, f1: 52 50 52 51 %\n",
            "run 2 on model nn3 with vectors holE_wnet_en\n",
            "\n",
            " lem2lem_entailment__1519.txt\n",
            "train (2735,), validate (151,), test (153,)\n",
            "fcs 4, dropouts 3\n",
            "Finished 24 epochs of training\n",
            "test result {'model': 'randpredict', 'total_examples': 153, 'threshold': 1.0, 'examples_above_threshold': 153, 'correct': 79, 'tp': 37, 'fp': 35, 'fn': 39}\n",
            "For randpredict on 153(100.00%) of 153 examples > 1.00 confidence, prec, rec, acc, f1: 51 48 51 50 %\n",
            "\n",
            "*** rel 15 of 27 ***\n",
            "\n",
            "hyponym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 16 of 27 ***\n",
            "\n",
            "category_domain lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 17 of 27 ***\n",
            "\n",
            "substance_meronym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 18 of 27 ***\n",
            "\n",
            "instance_hypernym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 19 of 27 ***\n",
            "\n",
            "member_of_usage_domain lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 20 of 27 ***\n",
            "\n",
            "instances_hyponym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 21 of 27 ***\n",
            "\n",
            "member_of_category_domain lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 22 of 27 ***\n",
            "\n",
            "also_see lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 23 of 27 ***\n",
            "\n",
            "part_holonym lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 24 of 27 ***\n",
            "\n",
            "usage_domain lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 25 of 27 ***\n",
            "\n",
            "member_of_region_domain lem2lem not in rel_name filter\n",
            "\n",
            "*** rel 26 of 27 ***\n",
            "\n",
            "member_meronym lem2lem not in rel_name filter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kwWRSH6ThXT",
        "colab_type": "text"
      },
      "source": [
        "`learn_results` is a list of trained models along with model metadata and evaluation results gathered during training and validation. It's a good idea to write these to disk:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FILYl2h-TOue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for lr in learn_results:\n",
        "  learn.store_learn_result(odir, lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCNbaYocQg2N",
        "colab_type": "text"
      },
      "source": [
        "The previous step will have generated a directory structure in the specified output dir. The structure is as follows:\n",
        "\n",
        "    (odir)/(rel_type)/(rel_name)/(emb_id)/(arch_id)/run_(number)/\n",
        "\n",
        "See the [embrela README](https://github.com/rdenaux/embrelassess) for more details about the generated files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fayvfNsRLlz",
        "colab_type": "text"
      },
      "source": [
        "## Analysing model results\n",
        "Now that we have trained (and evaluated) models for the selected datasets, we can load the results and analyse them. \n",
        "\n",
        "### Load and aggregate evaluation results for the trained models\n",
        "First we show how to load and aggregate evaluation data from disk once you have gone through the step of learning the models and you have a train result folder structure as described above. If you skipped that part, below we'll load pre-aggregated results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKosWJkXRmn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import embrelassess.analyse as analyse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqFmCEEOSKDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_read = learn.load_learn_results(odir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afpkbmlxUHaS",
        "colab_type": "text"
      },
      "source": [
        "We'll read the results from disk, aggregate the results and put them into a pandas DataFrame for easier analysis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSgJHuqGUAOw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ea73bbda-92f9-4d4e-e7fd-ab2671a144dc"
      },
      "source": [
        "import pandas as pd\n",
        "aggs = []\n",
        "for learn_result in lr_read:\n",
        "  rel_aggs = analyse.aggregate_runs(learn_result)\n",
        "  print('found', len(rel_aggs), 'for relation', learn_result['rel_name'])\n",
        "  aggs = aggs + rel_aggs\n",
        "        \n",
        "aggs_df = pd.DataFrame(aggs)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "found 4 for relation antonym\n",
            "found 4 for relation entailment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHeXqSKRm9m9",
        "colab_type": "text"
      },
      "source": [
        "We can inspect the resulting DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM3HphBgVDxM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "29215102-6113-4a84-9e93-1514bdecc45d"
      },
      "source": [
        "print(aggs_df.shape)\n",
        "aggs_df"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rel_type</th>\n",
              "      <th>rel_name</th>\n",
              "      <th>emb</th>\n",
              "      <th>acc_avg</th>\n",
              "      <th>acc_std</th>\n",
              "      <th>acc_min</th>\n",
              "      <th>acc_max</th>\n",
              "      <th>f1_avg</th>\n",
              "      <th>f1_std</th>\n",
              "      <th>f1_min</th>\n",
              "      <th>f1_max</th>\n",
              "      <th>precision_avg</th>\n",
              "      <th>precision_std</th>\n",
              "      <th>precision_min</th>\n",
              "      <th>precision_max</th>\n",
              "      <th>recall_avg</th>\n",
              "      <th>recall_std</th>\n",
              "      <th>recall_min</th>\n",
              "      <th>recall_max</th>\n",
              "      <th>model</th>\n",
              "      <th>datapoints</th>\n",
              "      <th>result_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>antonym</td>\n",
              "      <td>holE_wnet_en</td>\n",
              "      <td>0.497139</td>\n",
              "      <td>0.008093</td>\n",
              "      <td>0.491416</td>\n",
              "      <td>0.508584</td>\n",
              "      <td>0.439329</td>\n",
              "      <td>0.310652</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.658993</td>\n",
              "      <td>0.327611</td>\n",
              "      <td>0.231656</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.491416</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.471405</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>antonym</td>\n",
              "      <td>holE_wnet_en</td>\n",
              "      <td>0.501431</td>\n",
              "      <td>0.008280</td>\n",
              "      <td>0.493562</td>\n",
              "      <td>0.512876</td>\n",
              "      <td>0.492722</td>\n",
              "      <td>0.008425</td>\n",
              "      <td>0.484716</td>\n",
              "      <td>0.504367</td>\n",
              "      <td>0.492722</td>\n",
              "      <td>0.008425</td>\n",
              "      <td>0.484716</td>\n",
              "      <td>0.504367</td>\n",
              "      <td>0.492722</td>\n",
              "      <td>0.008425</td>\n",
              "      <td>0.484716</td>\n",
              "      <td>0.504367</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>antonym</td>\n",
              "      <td>rand_en</td>\n",
              "      <td>0.512160</td>\n",
              "      <td>0.003647</td>\n",
              "      <td>0.508584</td>\n",
              "      <td>0.517167</td>\n",
              "      <td>0.482729</td>\n",
              "      <td>0.009687</td>\n",
              "      <td>0.472222</td>\n",
              "      <td>0.495595</td>\n",
              "      <td>0.504089</td>\n",
              "      <td>0.004164</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.509804</td>\n",
              "      <td>0.463610</td>\n",
              "      <td>0.019878</td>\n",
              "      <td>0.445415</td>\n",
              "      <td>0.491266</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>antonym</td>\n",
              "      <td>rand_en</td>\n",
              "      <td>0.482117</td>\n",
              "      <td>0.019379</td>\n",
              "      <td>0.457082</td>\n",
              "      <td>0.504292</td>\n",
              "      <td>0.473071</td>\n",
              "      <td>0.019718</td>\n",
              "      <td>0.447598</td>\n",
              "      <td>0.495633</td>\n",
              "      <td>0.473071</td>\n",
              "      <td>0.019718</td>\n",
              "      <td>0.447598</td>\n",
              "      <td>0.495633</td>\n",
              "      <td>0.473071</td>\n",
              "      <td>0.019718</td>\n",
              "      <td>0.447598</td>\n",
              "      <td>0.495633</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>entailment</td>\n",
              "      <td>holE_wnet_en</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>0.663755</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.663755</td>\n",
              "      <td>0.663755</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>0.496732</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>entailment</td>\n",
              "      <td>holE_wnet_en</td>\n",
              "      <td>0.516340</td>\n",
              "      <td>0.010673</td>\n",
              "      <td>0.503268</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.011034</td>\n",
              "      <td>0.486486</td>\n",
              "      <td>0.513513</td>\n",
              "      <td>0.513889</td>\n",
              "      <td>0.011340</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.527778</td>\n",
              "      <td>0.486842</td>\n",
              "      <td>0.010743</td>\n",
              "      <td>0.473684</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>entailment</td>\n",
              "      <td>rand_en</td>\n",
              "      <td>0.625272</td>\n",
              "      <td>0.008152</td>\n",
              "      <td>0.614379</td>\n",
              "      <td>0.633987</td>\n",
              "      <td>0.599856</td>\n",
              "      <td>0.013051</td>\n",
              "      <td>0.581560</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.638503</td>\n",
              "      <td>0.006676</td>\n",
              "      <td>0.630769</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.565789</td>\n",
              "      <td>0.018608</td>\n",
              "      <td>0.539474</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>lem2lem</td>\n",
              "      <td>entailment</td>\n",
              "      <td>rand_en</td>\n",
              "      <td>0.481481</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.450980</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>0.463964</td>\n",
              "      <td>0.035469</td>\n",
              "      <td>0.432432</td>\n",
              "      <td>0.513513</td>\n",
              "      <td>0.476852</td>\n",
              "      <td>0.036454</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.527778</td>\n",
              "      <td>0.451754</td>\n",
              "      <td>0.034535</td>\n",
              "      <td>0.421053</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>nn3</td>\n",
              "      <td>3</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  rel_type    rel_name           emb  ...  model  datapoints  result_type\n",
              "0  lem2lem     antonym  holE_wnet_en  ...    nn3           3         test\n",
              "1  lem2lem     antonym  holE_wnet_en  ...    nn3           3       random\n",
              "2  lem2lem     antonym       rand_en  ...    nn3           3         test\n",
              "3  lem2lem     antonym       rand_en  ...    nn3           3       random\n",
              "4  lem2lem  entailment  holE_wnet_en  ...    nn3           3         test\n",
              "5  lem2lem  entailment  holE_wnet_en  ...    nn3           3       random\n",
              "6  lem2lem  entailment       rand_en  ...    nn3           3         test\n",
              "7  lem2lem  entailment       rand_en  ...    nn3           3       random\n",
              "\n",
              "[8 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcXiYxhJnIzc",
        "colab_type": "text"
      },
      "source": [
        "As we can see the DataFrame contains 22 columns, including `rel_type`, `rel_name`, `emb` and `model`, which identify the model that was trained and on which relation dataset. \n",
        "\n",
        "You will notice that for each combination we have two sets of results, but they have a different value for column `result_type`:\n",
        "* value `test` indicates results after training\n",
        "* value `random` indicates baseline results when testing using a heuristic choosing a label at random.\n",
        "\n",
        "As you can see, the training in this case does not produce very interesting results since there is no overlap between the words in the datasets and the HolE embeddings. To illustrate the analysis of results, we include results obtained by training on the same datasets using various embeddings. Below, we load the results into a DataFrame and continue analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9AK3_vP2HNU",
        "colab_type": "text"
      },
      "source": [
        "### Loading pre-aggregated results\n",
        "Since training models on a variety of embedding spaces and relations take a long while, here we'll load pre-aggregated results which we'll use to demonstrate how to analyse the data.\n",
        "We load two sets of aggregated results:\n",
        "* `aggregated_wn_results` contains results on training with the wnet datasets shown at the beginning of this notebook\n",
        "* `aggregated_random_dataset_results.tsv` contain results of training a variety of models/embeddings on datasets generated at random. We use this to detect **baseline ranges for prediction metrics** as discussed below.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaRxaBr0EqZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "wnet_results_df = pd.read_csv('embrela_prj/eval_data/aggregated_wn_results.tsv')\n",
        "rand_ds_results_df = pd.read_csv('embrela_prj/eval_data/aggregated_random_dataset_results.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW_3YViMPkOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df = pd.concat([wnet_results_df, rand_ds_results_df])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paiIUOI2PyE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d965857-3437-4f5b-b9e9-dbd8dac1fd37"
      },
      "source": [
        "aggs_df.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(788, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHuAHyo9FGF9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "35e33e5b-b898-4a69-ea11-371946833e0c"
      },
      "source": [
        "aggs_df.sample(n=5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>acc_avg</th>\n",
              "      <th>acc_max</th>\n",
              "      <th>acc_min</th>\n",
              "      <th>acc_std</th>\n",
              "      <th>datapoints</th>\n",
              "      <th>emb</th>\n",
              "      <th>f1_avg</th>\n",
              "      <th>f1_max</th>\n",
              "      <th>f1_min</th>\n",
              "      <th>f1_std</th>\n",
              "      <th>model</th>\n",
              "      <th>precision_avg</th>\n",
              "      <th>precision_max</th>\n",
              "      <th>precision_min</th>\n",
              "      <th>precision_std</th>\n",
              "      <th>recall_avg</th>\n",
              "      <th>recall_max</th>\n",
              "      <th>recall_min</th>\n",
              "      <th>recall_std</th>\n",
              "      <th>rel_name</th>\n",
              "      <th>rel_type</th>\n",
              "      <th>result_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>265</td>\n",
              "      <td>0.494737</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>0.447368</td>\n",
              "      <td>0.058608</td>\n",
              "      <td>5</td>\n",
              "      <td>swiv_UN_en</td>\n",
              "      <td>0.390250</td>\n",
              "      <td>0.618182</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.275576</td>\n",
              "      <td>nn2</td>\n",
              "      <td>0.467834</td>\n",
              "      <td>0.999990</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.317315</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.466749</td>\n",
              "      <td>substance_meronym</td>\n",
              "      <td>lem2lem</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>359</td>\n",
              "      <td>0.587302</td>\n",
              "      <td>0.619048</td>\n",
              "      <td>0.523810</td>\n",
              "      <td>0.044896</td>\n",
              "      <td>3</td>\n",
              "      <td>vecsi_un_en</td>\n",
              "      <td>0.638889</td>\n",
              "      <td>0.666666</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.039284</td>\n",
              "      <td>nn3</td>\n",
              "      <td>0.589743</td>\n",
              "      <td>0.615384</td>\n",
              "      <td>0.538461</td>\n",
              "      <td>0.036262</td>\n",
              "      <td>0.696969</td>\n",
              "      <td>0.727272</td>\n",
              "      <td>0.636363</td>\n",
              "      <td>0.042855</td>\n",
              "      <td>random_200</td>\n",
              "      <td>syn2syn</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>616</td>\n",
              "      <td>0.460317</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.022448</td>\n",
              "      <td>3</td>\n",
              "      <td>vecsi_wiki_en</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.282843</td>\n",
              "      <td>nn3</td>\n",
              "      <td>0.157895</td>\n",
              "      <td>0.473684</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.223297</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.818181</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.385694</td>\n",
              "      <td>random_200</td>\n",
              "      <td>lem2lem</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>402</td>\n",
              "      <td>0.487988</td>\n",
              "      <td>0.524181</td>\n",
              "      <td>0.455538</td>\n",
              "      <td>0.023893</td>\n",
              "      <td>5</td>\n",
              "      <td>glove_cc_en</td>\n",
              "      <td>0.494299</td>\n",
              "      <td>0.530046</td>\n",
              "      <td>0.462250</td>\n",
              "      <td>0.023598</td>\n",
              "      <td>nn3</td>\n",
              "      <td>0.474556</td>\n",
              "      <td>0.508876</td>\n",
              "      <td>0.443787</td>\n",
              "      <td>0.022656</td>\n",
              "      <td>0.515756</td>\n",
              "      <td>0.553055</td>\n",
              "      <td>0.482315</td>\n",
              "      <td>0.024623</td>\n",
              "      <td>part_meronym</td>\n",
              "      <td>lem2lem</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472</th>\n",
              "      <td>12</td>\n",
              "      <td>0.760900</td>\n",
              "      <td>0.772152</td>\n",
              "      <td>0.751055</td>\n",
              "      <td>0.008670</td>\n",
              "      <td>3</td>\n",
              "      <td>glove_cc_en</td>\n",
              "      <td>0.777252</td>\n",
              "      <td>0.784906</td>\n",
              "      <td>0.771186</td>\n",
              "      <td>0.005712</td>\n",
              "      <td>nn2</td>\n",
              "      <td>0.706402</td>\n",
              "      <td>0.745902</td>\n",
              "      <td>0.684564</td>\n",
              "      <td>0.027982</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.912281</td>\n",
              "      <td>0.798246</td>\n",
              "      <td>0.050136</td>\n",
              "      <td>instance_hypernym</td>\n",
              "      <td>lem2lem</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0   acc_avg   acc_max  ...           rel_name  rel_type  result_type\n",
              "265         265  0.494737  0.578947  ...  substance_meronym   lem2lem         test\n",
              "211         359  0.587302  0.619048  ...         random_200   syn2syn       random\n",
              "96          616  0.460317  0.476190  ...         random_200   lem2lem         test\n",
              "402         402  0.487988  0.524181  ...       part_meronym   lem2lem       random\n",
              "472          12  0.760900  0.772152  ...  instance_hypernym   lem2lem         test\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUrJEmRNFPex",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the data read from the tsv has the same columns as we would get by aggregating the results read from disk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0PNWokSFrqW",
        "colab_type": "text"
      },
      "source": [
        "## Data pre-processing: combine and add fields\n",
        "Most of the columns in the DataFrame are aggregated values, but for further analysis it's useful to combine fields and relation metadata.\n",
        "\n",
        "First we add column `rel_name_type`, which combines the `rel_name` and `rel_type`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m24QXudXFlN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df['rel_name_type'] = aggs_df.apply(lambda row: '%s_%s' % (row['rel_name'], row['rel_type']), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ7z0e2mGkuW",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to fix some terminology issues. We rename column `rel_type` to `pair_type` because values like `lem2lem` describes the types of pairs we tried to predict. We use `rel_type` to create high-level relation types like `similarity` and `lexical`,  as described in the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dus7KSW1GtuA",
        "colab_type": "text"
      },
      "source": [
        "We'll add a column 'KG' to document that all the relations came from WordNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfgTa2JfG8R9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df['KG'] = 'wnet'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcyIidhVGiPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df.rename(columns={'rel_type': 'pair_type'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg_eFwCKIOrC",
        "colab_type": "text"
      },
      "source": [
        "The `embrela` project defines a utility script `relutil` which calculates the relation type based on the rows in the DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrDOrvxdHu-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp embrela_prj/relutil.py ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIQXStN4H-jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import relutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5zxdKytICIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df['rel_type'] = aggs_df.apply(\n",
        "    relutil.calc_rel_type_for_dfrow_fn(rel_name_field='rel_name'), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hWzzVzmIqst",
        "colab_type": "text"
      },
      "source": [
        "We also add column `emb_corpus_type` to distinguish between types of embeddings used:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB6O21sAI4CM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_emb_corpus_type(row):\n",
        "    emb=row['emb']\n",
        "    if emb.startswith('holE_'):\n",
        "        return 'kg'\n",
        "    elif emb.startswith('rand_'):\n",
        "        return 'no-corpus'\n",
        "    elif emb.startswith('swivelsyn_'):\n",
        "        return 'concept-corpus'\n",
        "    elif emb.startswith('ftsyn_'):\n",
        "        return 'concept-corpus'\n",
        "    elif emb.startswith('vecsi_'):\n",
        "        return 'word-concept-corpus'\n",
        "    elif emb.startswith('sw2v_umbc'):\n",
        "        return 'word-corpus'\n",
        "    else:\n",
        "        return 'word-corpus'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEjs9KdhI6So",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df['emb_corpus_type'] = aggs_df.apply(calc_emb_corpus_type, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHi7D95OJJMr",
        "colab_type": "text"
      },
      "source": [
        "Now we start merging the relation dataset metadata into the aggregated DataFrame. We first add a field `positive_examples`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht-QClO9JTqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_positive_examples(row):\n",
        "    name = row['rel_name']\n",
        "    ptype = row['pair_type']\n",
        "    \n",
        "    name_filter = rels_df['name'] == name\n",
        "    ptype_filter = rels_df['type'] == ptype\n",
        "    cnts = rels_df[name_filter & ptype_filter]['cnt'].values\n",
        "    if len(cnts) > 0:\n",
        "      return cnts[0]\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0n5PU6NJoP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df['positive_examples'] = aggs_df.apply(calc_positive_examples, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0cyNddrJ2By",
        "colab_type": "text"
      },
      "source": [
        "## Calculate the range thresholds and biased dataset detection\n",
        "Next, we can calculate range thresholds as described in the paper.  $\\tau^{\\upsilon_\\mathrm{min}}_{\\mathrm{biased}} = \\mu^\\upsilon_{\\delta_{\\mathtt{rand},x}} - 2 \\sigma^\\upsilon_{\\delta{\\mathrm{rand},x}}$ and $\\tau^{\\upsilon_\\mathrm{max}}_{\\mathrm{biased}} = \\mu^\\upsilon_{\\delta_{\\mathtt{rand},x}} + 2 \\sigma^\\upsilon_{\\delta{\\mathrm{rand},x}}$. Any metrics within these ranges could be due to chance with 95\\% probability because several models trained on datasets with random pairs achieve values within these ranges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbrtR_J9K9Mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randomrel_filter = aggs_df['rel_name'].str.startswith('random_')\n",
        "\n",
        "def calc_rand_avg_and_std(metric='f1', debug=False):    \n",
        "    test_result_filter = aggs_df['result_type']=='test'\n",
        "\n",
        "    avg_field = '%s_avg' % metric\n",
        "    std_field = '%s_std' % metric\n",
        "    \n",
        "    sub_df = aggs_df[randomrel_filter & test_result_filter]\n",
        "    if debug:\n",
        "        print('calculating averages over %d measurements' % sub_df.shape[0])\n",
        "        print('rels', sub_df['rel_name'].unique())\n",
        "        print('embs', sub_df['emb'].unique())\n",
        "        sub_df[avg_field].hist()\n",
        "    rand_metric_avg2 = sub_df[avg_field].mean()\n",
        "    rand_metric_avg_std = sub_df[avg_field].std()\n",
        "    rand_metric_std_avg = sub_df[std_field].mean()\n",
        "    max_std = max(rand_metric_avg_std, rand_metric_std_avg)\n",
        "    \n",
        "    return {'avg': rand_metric_avg2, 'std': max_std}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqlt-boaLkzO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "f2bb8974-8c20-4622-a7e5-05ddf7a6be5a"
      },
      "source": [
        "calc_rand_avg_and_std(debug=True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "calculating averages over 126 measurements\n",
            "rels ['random_1000' 'random_10000' 'random_5000' 'random_200' 'random_500'\n",
            " 'random_50000']\n",
            "embs ['glove_un_en' 'swivel_wiki_en' 'holE_sensi_en' 'ft_un_en' 'ft_cc_en'\n",
            " 'vecsi_un_en' 'glove_wiki_en' 'glove_cc_en' 'ft_wiki_en' 'vecsi_wiki_en'\n",
            " 'rand_en' 'swivel_un_en' 'vecsi_un_en_ts' 'swivelsyn_wiki_en_25e'\n",
            " 'ftsyn_wiki' 'swivelsyn_wiki_en_8e']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'avg': 0.4070040067097273, 'std': 0.12301846272363581}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-XFsvLB4BQP",
        "colab_type": "text"
      },
      "source": [
        "As we can see, models trained on datasets of random pairs obtain values $f1$ metrics with\n",
        "\n",
        "$\\mu^\\upsilon_{\\delta_{\\mathtt{rand}}} = 0.407$  and\n",
        "\n",
        "$\\sigma^\\upsilon_{\\delta_{\\mathtt{rand}}} = 0.123$\n",
        "\n",
        "This gives us:\n",
        "\n",
        "$\\tau^{\\upsilon_\\mathrm{min}}_{\\mathrm{biased}} = 0.161$ and\n",
        "\n",
        "$\\tau^{\\upsilon_\\mathrm{max}}_{\\mathrm{biased}} = 0.653$\n",
        "\n",
        "Any results within this range have a 95% probability of being the result of chance and are not necessarily due to the used embeddings encoding relational knowledge about the relation being predicted.\n",
        "\n",
        "We define two filter biased test results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSgRX2M6SEcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def partition_rel_name_types_at_mu_plus_sigma(sigma_factor=2.0, metric='f1'):\n",
        "    randemb_filter = aggs_df['emb'] == 'rand_en'\n",
        "    result = {'over': [], 'under': []}\n",
        "    avg_std = calc_rand_avg_and_std(metric=metric)\n",
        "    avg_threshold = avg_std['avg'] + sigma_factor * avg_std['std']\n",
        "    avg_field = '%s_avg' % metric\n",
        "    rnts = list(aggs_df['rel_name_type'].unique())\n",
        "    for rnt in rnts:\n",
        "        rnt_filter = aggs_df['rel_name_type'] == rnt\n",
        "        randemb_df = aggs_df[rnt_filter & test_result_filter() & randemb_filter]\n",
        "        randemb_avg = randemb_df[avg_field].mean()\n",
        "        if randemb_avg > avg_threshold:\n",
        "            result['over'].append(rnt)\n",
        "        else:\n",
        "            result['under'].append(rnt)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUrOTWrl60wP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_biased_rel_types(sigma_factor=2.0, metric='f1'):\n",
        "    rnt_partition_over = partition_rel_name_types_at_mu_plus_sigma(sigma_factor=sigma_factor, metric=metric)\n",
        "    rnt_partition_under = partition_rel_name_types_at_mu_plus_sigma(sigma_factor=-sigma_factor, metric=metric)\n",
        "    valid_rnts = set.intersection(set(rnt_partition_over['under']), set(rnt_partition_under['over']))\n",
        "    return aggs_df['rel_name_type'].isin(valid_rnts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNfwDK9q7M9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_result_filter():\n",
        "    return aggs_df['result_type']=='test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbSdPYIE7Cjd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "d8ec96ce-86b2-42c3-f104-768962075c3e"
      },
      "source": [
        "aggs_df[filter_biased_rel_types() & test_result_filter()].sample(n=5)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>acc_avg</th>\n",
              "      <th>acc_max</th>\n",
              "      <th>acc_min</th>\n",
              "      <th>acc_std</th>\n",
              "      <th>datapoints</th>\n",
              "      <th>emb</th>\n",
              "      <th>f1_avg</th>\n",
              "      <th>f1_max</th>\n",
              "      <th>f1_min</th>\n",
              "      <th>f1_std</th>\n",
              "      <th>model</th>\n",
              "      <th>precision_avg</th>\n",
              "      <th>precision_max</th>\n",
              "      <th>precision_min</th>\n",
              "      <th>precision_std</th>\n",
              "      <th>recall_avg</th>\n",
              "      <th>recall_max</th>\n",
              "      <th>recall_min</th>\n",
              "      <th>recall_std</th>\n",
              "      <th>rel_name</th>\n",
              "      <th>pair_type</th>\n",
              "      <th>result_type</th>\n",
              "      <th>rel_name_type</th>\n",
              "      <th>KG</th>\n",
              "      <th>rel_type</th>\n",
              "      <th>emb_corpus_type</th>\n",
              "      <th>positive_examples</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>60</td>\n",
              "      <td>0.470000</td>\n",
              "      <td>0.490000</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.021602</td>\n",
              "      <td>3</td>\n",
              "      <td>ft_cc_en</td>\n",
              "      <td>0.438313</td>\n",
              "      <td>0.446808</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.007497</td>\n",
              "      <td>nn3</td>\n",
              "      <td>0.457143</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.020574</td>\n",
              "      <td>0.421769</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.408163</td>\n",
              "      <td>0.009620</td>\n",
              "      <td>random_1000</td>\n",
              "      <td>lem2lem</td>\n",
              "      <td>test</td>\n",
              "      <td>random_1000_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>random</td>\n",
              "      <td>word-corpus</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>1188</td>\n",
              "      <td>0.492168</td>\n",
              "      <td>0.499900</td>\n",
              "      <td>0.487702</td>\n",
              "      <td>0.005489</td>\n",
              "      <td>3</td>\n",
              "      <td>vecsi_wiki_en</td>\n",
              "      <td>0.533611</td>\n",
              "      <td>0.656728</td>\n",
              "      <td>0.396913</td>\n",
              "      <td>0.106503</td>\n",
              "      <td>nn3</td>\n",
              "      <td>0.484748</td>\n",
              "      <td>0.488902</td>\n",
              "      <td>0.481793</td>\n",
              "      <td>0.003024</td>\n",
              "      <td>0.656578</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.336605</td>\n",
              "      <td>0.271337</td>\n",
              "      <td>random_50000</td>\n",
              "      <td>lem2lem</td>\n",
              "      <td>test</td>\n",
              "      <td>random_50000_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>random</td>\n",
              "      <td>word-concept-corpus</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>92</td>\n",
              "      <td>0.702222</td>\n",
              "      <td>0.723232</td>\n",
              "      <td>0.682828</td>\n",
              "      <td>0.015680</td>\n",
              "      <td>5</td>\n",
              "      <td>vecsi_UN_en</td>\n",
              "      <td>0.674545</td>\n",
              "      <td>0.717526</td>\n",
              "      <td>0.557471</td>\n",
              "      <td>0.059335</td>\n",
              "      <td>nn3</td>\n",
              "      <td>0.695830</td>\n",
              "      <td>0.822034</td>\n",
              "      <td>0.618892</td>\n",
              "      <td>0.069702</td>\n",
              "      <td>0.687826</td>\n",
              "      <td>0.826087</td>\n",
              "      <td>0.421739</td>\n",
              "      <td>0.142099</td>\n",
              "      <td>verb_group</td>\n",
              "      <td>lem2lem</td>\n",
              "      <td>test</td>\n",
              "      <td>verb_group_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>similarity</td>\n",
              "      <td>word-concept-corpus</td>\n",
              "      <td>4944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>340</td>\n",
              "      <td>0.557895</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.541353</td>\n",
              "      <td>0.012030</td>\n",
              "      <td>5</td>\n",
              "      <td>glove_cc_en</td>\n",
              "      <td>0.530335</td>\n",
              "      <td>0.601399</td>\n",
              "      <td>0.435644</td>\n",
              "      <td>0.060065</td>\n",
              "      <td>nn3</td>\n",
              "      <td>0.565661</td>\n",
              "      <td>0.628571</td>\n",
              "      <td>0.538461</td>\n",
              "      <td>0.032331</td>\n",
              "      <td>0.518182</td>\n",
              "      <td>0.651515</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.118298</td>\n",
              "      <td>member_meronym</td>\n",
              "      <td>lem2lem</td>\n",
              "      <td>test</td>\n",
              "      <td>member_meronym_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>meronymy</td>\n",
              "      <td>word-corpus</td>\n",
              "      <td>1315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>0.536986</td>\n",
              "      <td>0.589041</td>\n",
              "      <td>0.506849</td>\n",
              "      <td>0.027940</td>\n",
              "      <td>5</td>\n",
              "      <td>glove_cc_en</td>\n",
              "      <td>0.594779</td>\n",
              "      <td>0.613636</td>\n",
              "      <td>0.581395</td>\n",
              "      <td>0.012458</td>\n",
              "      <td>nn3</td>\n",
              "      <td>0.514249</td>\n",
              "      <td>0.560975</td>\n",
              "      <td>0.490196</td>\n",
              "      <td>0.024500</td>\n",
              "      <td>0.708571</td>\n",
              "      <td>0.771428</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.037904</td>\n",
              "      <td>cause</td>\n",
              "      <td>lem2lem</td>\n",
              "      <td>test</td>\n",
              "      <td>cause_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>similarity</td>\n",
              "      <td>word-corpus</td>\n",
              "      <td>719</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0   acc_avg  ...      emb_corpus_type  positive_examples\n",
              "8            60  0.470000  ...          word-corpus                  0\n",
              "148        1188  0.492168  ...  word-concept-corpus                  0\n",
              "92           92  0.702222  ...  word-concept-corpus               4944\n",
              "340         340  0.557895  ...          word-corpus               1315\n",
              "20           20  0.536986  ...          word-corpus                719\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKpqx3ma804n",
        "colab_type": "text"
      },
      "source": [
        "## Finding statistically significant models\n",
        " We consider a dataset to be **biased** if models can perform well on them regardless of whether the embeddings used to train the model encode any information. Intuitively, these are datasets which are imbalanced in some way allowing the model to exploit this imbalance during prediction, but that do not reflect the knowledge encoded in the embeddings. To detect these, we look at model results for models trained on random embeddings (i.e. on models $m^{\\delta_r,s_{\\mathrm{rand}},t}$). We say that $\\delta_r$ is biased if $\\mu^\\mathrm{f1}_{\\delta_r, s_{\\mathrm{rand}}}$ is outside of the  $[ \\tau^{\\mathrm{f1}_\\mathrm{min}}_{\\mathrm{biased}}, \\tau^{\\mathrm{f1}_\\mathrm{max}}_{\\mathrm{biased}} ]$ range. The rationale is that even with random embeddings, such models were able to perform outside of the 95\\% baseline ranges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GtsVeu97_7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_delta_to_rand_fn(whole_df, metric_field='f1_avg'):\n",
        "    def calc_delta_to_rand(row):\n",
        "        rel_name_type = row['rel_name_type']\n",
        "        emb = row['emb']\n",
        "        result_type = row['result_type']\n",
        "        row_metric = row[metric_field]        \n",
        "        \n",
        "        rel_filter = whole_df['rel_name_type'] == rel_name_type\n",
        "        rand_filter = whole_df['emb'] == 'rand_en'\n",
        "        result_type_filter = whole_df['result_type'] == result_type\n",
        "        \n",
        "        rand_metrics = whole_df[rel_filter & result_type_filter & rand_filter][metric_field].values\n",
        "        #assert len(rand_metrics) == 1, '%s has %s rand_metrics' % (rel_name_type, rand_metrics)\n",
        "        rand_metric = rand_metrics[0]\n",
        "        \n",
        "        #print('rand_metric type', type(rand_metric))\n",
        "        delta = row_metric - rand_metric\n",
        "        #print('for rel %s, delta for emb %s %s' % (rel_name_type, emb, delta))\n",
        "        return delta\n",
        "    return calc_delta_to_rand"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDLeS_Lc9eoE",
        "colab_type": "text"
      },
      "source": [
        "We can store the delta between the model and the random predictions as a field in our DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPtCkFQO9SHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggs_df['delta_f1_to_rand_emb'] = aggs_df.apply(calc_delta_to_rand_fn(aggs_df), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtXd6G_v9b_Y",
        "colab_type": "text"
      },
      "source": [
        "And we specify this delta in terms of the $\\sigma$, which we'll store in another column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXalbk_d96qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_sigmadelta_to_rand_fn(whole_df, metric='f1'):\n",
        "    avg_metric_field = '%s_avg' % metric\n",
        "    std_metric_field = '%s_std' % metric\n",
        "    def calc_sigmadelta_to_rand(row):\n",
        "        rel_name_type = row['rel_name_type']\n",
        "        emb = row['emb']\n",
        "        result_type = row['result_type']\n",
        "        row_avg_metric = row[avg_metric_field]\n",
        "        row_std_metric = row[std_metric_field]\n",
        "        \n",
        "        rel_filter = whole_df['rel_name_type'] == rel_name_type\n",
        "        rand_filter = whole_df['emb'] == 'rand_en'\n",
        "        result_type_filter = whole_df['result_type'] == result_type\n",
        "        \n",
        "        rand_df = whole_df[rel_filter & result_type_filter & rand_filter]\n",
        "        rand_avg_metric = rand_df[avg_metric_field].values[0]\n",
        "        rand_std_metric = rand_df[std_metric_field].values[0]\n",
        "        \n",
        "        max_std = max(row_std_metric, rand_std_metric)\n",
        "        \n",
        "        #print('rand_metric type', type(rand_metric))\n",
        "        delta = row_avg_metric - rand_avg_metric\n",
        "        sigma_delta = delta / max_std\n",
        "        return sigma_delta\n",
        "    return calc_sigmadelta_to_rand"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNLN9Ov39_en",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0540a185-781a-4dd3-949e-e67d45826c35"
      },
      "source": [
        "aggs_df['sigdelta_f1_to_rand_emb'] = aggs_df.apply(calc_sigmadelta_to_rand_fn(aggs_df), axis=1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBsOIRud-HOW",
        "colab_type": "text"
      },
      "source": [
        "This field allows us to define filters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRNsbe8p-BDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigdelta_over(value):\n",
        "    return aggs_df['sigdelta_f1_to_rand_emb'] > value\n",
        "def sigdelta_under(value):\n",
        "    return aggs_df['sigdelta_f1_to_rand_emb'] < value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiSP4ds-fom",
        "colab_type": "text"
      },
      "source": [
        "Which we can use to find which embeddings resulted in models with statistically better results than their random counterparts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3NSRr3K-Nth",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "99ef0857-1d27-437d-ccec-d9c66572e412"
      },
      "source": [
        "aggs_df[test_result_filter() & sigdelta_over(2.0)]['emb'].unique()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['glove_cc_en', 'ft_wiki_en', 'vecsi_UN_en', 'swiv_UN_en',\n",
              "       'ft_wikip_en', 'vecsi_wiki_en', 'vecsi_un_en', 'holE_sensi_en',\n",
              "       'ft_cc_en', 'ft_un_en', 'swivel_un_en', 'vecsi_un_en_ts',\n",
              "       'swivel_wiki_en', 'ftsyn_wiki'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVSrrDsT-7GK",
        "colab_type": "text"
      },
      "source": [
        "## Combining filters\n",
        "\n",
        "Now we have seen filters for:\n",
        "* detecting biased datasets\n",
        "* detecting statistically significant results\n",
        "\n",
        "We have further filters for only selecting rows describing test results and results that were trained on non-random datasets. For example, to select only those results which significantly outperformed random predictions, we can use the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWba6vOm-Pi5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "33401509-c17e-4381-aa07-3338959490e9"
      },
      "source": [
        "display_columns = ['datapoints', 'emb', 'f1_avg', 'model', 'rel_name_type', 'KG', 'positive_examples', 'sigdelta_f1_to_rand_emb']\n",
        "aggs_df[test_result_filter() & filter_biased_rel_types() & sigdelta_over(2.0) & ~randomrel_filter][display_columns]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datapoints</th>\n",
              "      <th>emb</th>\n",
              "      <th>f1_avg</th>\n",
              "      <th>model</th>\n",
              "      <th>rel_name_type</th>\n",
              "      <th>KG</th>\n",
              "      <th>positive_examples</th>\n",
              "      <th>sigdelta_f1_to_rand_emb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>glove_cc_en</td>\n",
              "      <td>0.759442</td>\n",
              "      <td>nn3</td>\n",
              "      <td>hypernym_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>110650</td>\n",
              "      <td>12.220272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>glove_cc_en</td>\n",
              "      <td>0.760317</td>\n",
              "      <td>nn2</td>\n",
              "      <td>hypernym_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>110650</td>\n",
              "      <td>12.317966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>5</td>\n",
              "      <td>ft_wiki_en</td>\n",
              "      <td>0.759657</td>\n",
              "      <td>nn3</td>\n",
              "      <td>hypernym_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>110650</td>\n",
              "      <td>12.244312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5</td>\n",
              "      <td>ft_wiki_en</td>\n",
              "      <td>0.762105</td>\n",
              "      <td>nn2</td>\n",
              "      <td>hypernym_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>110650</td>\n",
              "      <td>12.517522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>vecsi_UN_en</td>\n",
              "      <td>0.758949</td>\n",
              "      <td>nn3</td>\n",
              "      <td>hypernym_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>110650</td>\n",
              "      <td>12.165311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>3</td>\n",
              "      <td>vecsi_un_en</td>\n",
              "      <td>0.668137</td>\n",
              "      <td>nn2</td>\n",
              "      <td>entailment_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>1519</td>\n",
              "      <td>3.217797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>5</td>\n",
              "      <td>vecsi_un_en</td>\n",
              "      <td>0.688376</td>\n",
              "      <td>nn2</td>\n",
              "      <td>verb_group_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>4944</td>\n",
              "      <td>3.772919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>5</td>\n",
              "      <td>glove_cc_en</td>\n",
              "      <td>0.659395</td>\n",
              "      <td>nn2</td>\n",
              "      <td>verb_group_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>4944</td>\n",
              "      <td>3.117599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>520</th>\n",
              "      <td>5</td>\n",
              "      <td>holE_sensi_en</td>\n",
              "      <td>0.656969</td>\n",
              "      <td>nn2</td>\n",
              "      <td>entailment_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>1519</td>\n",
              "      <td>2.125610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524</th>\n",
              "      <td>5</td>\n",
              "      <td>vecsi_un_en</td>\n",
              "      <td>0.675007</td>\n",
              "      <td>nn2</td>\n",
              "      <td>entailment_lem2lem</td>\n",
              "      <td>wnet</td>\n",
              "      <td>1519</td>\n",
              "      <td>3.097962</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     datapoints            emb  ...  positive_examples sigdelta_f1_to_rand_emb\n",
              "0             5    glove_cc_en  ...             110650               12.220272\n",
              "1             5    glove_cc_en  ...             110650               12.317966\n",
              "8             5     ft_wiki_en  ...             110650               12.244312\n",
              "9             5     ft_wiki_en  ...             110650               12.517522\n",
              "12            5    vecsi_UN_en  ...             110650               12.165311\n",
              "..          ...            ...  ...                ...                     ...\n",
              "494           3    vecsi_un_en  ...               1519                3.217797\n",
              "508           5    vecsi_un_en  ...               4944                3.772919\n",
              "512           5    glove_cc_en  ...               4944                3.117599\n",
              "520           5  holE_sensi_en  ...               1519                2.125610\n",
              "524           5    vecsi_un_en  ...               1519                3.097962\n",
              "\n",
              "[65 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsVwVW_kAIjt",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook we practiced using the `embrela` library for assessing how well various word/concept embeddings capture relational knowledge in WordNet.\n",
        "\n",
        "The pipeline is straightforward, but takes into account a variety of pitfalls that can occur during dataset creation, training and interpreting the classification results.\n"
      ]
    }
  ]
}